<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml">
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    <title>Introductions (Review) and Several Preliminary Statistical Methods &#8212; Intelligence</title>
    <link rel="stylesheet" href="../../_static/basic.css" type="text/css" />
    <link rel="stylesheet" href="../../_static/pygments.css" type="text/css" />
    <link rel="stylesheet" href="../../_static/modify.css" type="text/css" />
    <link rel="stylesheet" href="../../_static/bootswatch-3.3.6/readable/bootstrap.min.css" type="text/css" />
    <link rel="stylesheet" href="../../_static/bootstrap-sphinx.css" type="text/css" />
    <script type="text/javascript">
      var DOCUMENTATION_OPTIONS = {
        URL_ROOT:    '../../',
        VERSION:     '0.0.8',
        COLLAPSE_INDEX: false,
        FILE_SUFFIX: '.html',
        HAS_SOURCE:  true,
        SOURCELINK_SUFFIX: '.txt'
      };
    </script>
    <script type="text/javascript" src="../../_static/jquery.js"></script>
    <script type="text/javascript" src="../../_static/underscore.js"></script>
    <script type="text/javascript" src="../../_static/doctools.js"></script>
    <script type="text/javascript" src="../../_static/js/jquery-1.11.0.min.js"></script>
    <script type="text/javascript" src="../../_static/js/jquery-fix.js"></script>
    <script type="text/javascript" src="../../_static/bootstrap-3.3.6/js/bootstrap.min.js"></script>
    <script type="text/javascript" src="../../_static/bootstrap-sphinx.js"></script>
    <link rel="shortcut icon" href="../../_static/favicon.png"/>
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
    <link rel="next" title="Linear Methods for Regression" href="linear-methods-for-regresssion.html" />
    <link rel="prev" title="The Element of Statistical Learning" href="index.html" />

<!-- <script type="text/x-mathjax-config">
MathJax.Hub.Config({ TeX: { extensions: ["color.js","cancel.js", "AMSmath.js", "AMSsymbols.js"] }});
</script>
<script type="text/x-mathjax-config">
MathJax.Hub.Register.StartupHook("TeX Jax Ready",function () {
MathJax.Hub.Insert(MathJax.InputJax.TeX.Definitions.macros,{
   cancel: ["Extension","cancel"]
   });
});
</script>
<script type="text/x-mathjax-config">
MathJax.Hub.Config({
   tex2jax: {
     skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
   }
});
</script>
<script type="text/x-mathjax-config">
MathJax.Hub.Queue(function() {
     var all = MathJax.Hub.getAllJax(), i;
     for(i=0; i < all.length; i += 1) {
         all[i].SourceElement().parentNode.className += ' has-jax';
     }
});
</script>
<script type="text/x-mathjax-config">
MathJax.Hub.Config({
tex2jax: {
   processEscapes: true
}
});
</script>
<script type="text/x-mathjax-config">
MathJax.Hub.Config({
"fast-preview": {
Chunks: {EqnChunk: 10000, EqnChunkFactor: 1, EqnChunkDelay: 0},
color: "inherit!important",
updateTime: 30, updateDelay: 6,
messageStyle: "none",
disabled: false
}
});
</script>
<script type="text/x-mathjax-config">
MathJax.Hub.Config({
TeX: {
   Macros: {
     overlr: ['\\overset\\leftrightarrow{\#1}',1],
     overl: ['\\overset\leftarrow{\#1}',1],
     overr: ['\\overset\rightarrow{\#1}',1],
     bra: ['\\left\\langle \#1\\right|',1],
     ket: ['\\left| \#1\\right\\rangle',1],
     braket: ['\\langle \#1 \\mid \#2 \\rangle',2],
     avg: ['\\left< \#1 \\right>',1],
     slashed: ['\\cancel{\#1}',1],
     bold: ['\\boldsymbol{\#1}',1],
     sech: ['\\operatorname{sech}{\#1}',1],
     csch: ['\\operatorname{csch}{\#1}',1],
     Tr: ['\\operatorname{Tr}{\#1}',1]
   }
}
});
</script>
<script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/MathJax.js?config=TeX-MML-AM_CHTML">
</script> -->






<!-- Toggle Environment Begin -->
  <script type="text/javascript">
    $(document).ready(function() {
        $(".toggle > *").hide();
        $(".toggle .admonition-title").show();
        $(".toggle .admonition-title").click(function() {
            $(this).parent().children().not(".admonition-title").toggle(400);
            $(this).parent().children(".admonition-title").toggleClass("open");
        })
    });
    </script>
<!-- Toggle Environment End -->



<meta charset='utf-8'>
<meta http-equiv='X-UA-Compatible' content='IE=edge,chrome=1'>
<meta name='viewport' content='width=device-width, initial-scale=1.0, maximum-scale=1'>
<meta name="apple-mobile-web-app-capable" content="yes">
 
  </head>
  <body>

  <div id="navbar" class="navbar navbar-inverse navbar-default ">
    <div class="container">
      <div class="navbar-header">
        <!-- .btn-navbar is used as the toggle for collapsed navbar content -->
        <button type="button" class="navbar-toggle" data-toggle="collapse" data-target=".nav-collapse">
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
        </button>
        <a class="navbar-brand" href="../../index.html"><span><img src="../../_static/intelligence.png"></span>
          Intelligence</a>
        <span class="navbar-text navbar-version pull-left"><b>0.0.8</b></span>
      </div>

        <div class="collapse navbar-collapse nav-collapse">
          <ul class="nav navbar-nav">
            
            
              <li class="dropdown globaltoc-container">
  <a role="button"
     id="dLabelGlobalToc"
     data-toggle="dropdown"
     data-target="#"
     href="../../index.html">Chapters <b class="caret"></b></a>
  <ul class="dropdown-menu globaltoc"
      role="menu"
      aria-labelledby="dLabelGlobalToc"><ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../../vocabulary.html">Vocabulary</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../questions.html">Questions? Yes</a></li>
<li class="toctree-l1 current"><a class="reference internal" href="../index.html">Statistics</a><ul class="current">
<li class="toctree-l2 current"><a class="reference internal" href="index.html">The Element of Statistical Learning</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../network/index.html">Network Dynamics</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../network/popularity-similarity.html">Popularity vs Similarity in Growing Networks</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../neuroscience/index.html">Neuroscience</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../neuroscience/biology.html">Biology</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../neuroscience/visual-cortex.html">Visual Cortex</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../neuroscience/memory/index.html">Memory</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../neuroscience/alzheimers-disease.html">Alzheimer’s Disease</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../neuroscience/functional-connectivity.html">Functional Connectivity</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../neuroscience/solitary-waves.html">Solitary Waves</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../neuroscience/signal-decomposition.html">Signal Decomposition</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../neuroscience/dispersion-relation.html">Dispersion Relation</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../neuroscience/lesion-of-network.html">Lesion of Network: Brain Injury, Tumor, etc</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../neuroscience/misc.html">MISC</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../neuroscience/glossary.html">Glossary</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../neuroscience-models/index.html">Neuroscience Models</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../neuroscience-models/synapses.html">Synapses</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../equation-solving-in-neuroscience/index.html">Equation Solving in Neuroscience</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../equation-solving-in-neuroscience/green-function.html">Green’s Function Method</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../equation-solving-in-neuroscience/fourier-transform.html">Fourier Tranform</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../equation-solving-in-neuroscience/laplace-transform.html">Laplace Tranform</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../random-walks/index.html">Random Walks</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../random-walks/random-walks.html">Random Walks</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../machine-intelligence/index.html">Machine Intelligence</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../machine-intelligence/ann.html">Artificial Neural Network</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../machine-intelligence/neural-network-and-fem.html">Neural Networks and Finite Element Method</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../machine-intelligence/boltzmann-machine.html">Boltzmann Machine</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../machine-intelligence/lda.html">LDA Algorithm</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../collective/index.html">Collective Intelligence</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../collective/refs.html">Collective intelligence References</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../topics/index.html">Some Topics</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../topics/mean-field.html">Mean-field Method</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../topics/spin-glass.html">Spin Glass</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../topics/binary-human-model.html">Binary Human Model</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../topics/factor-graph.html">Factor Graph</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../topics/cosmic-rays-and-neuroscience.html">Cosmic Rays and Neuroscience</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../random-thoughts.html">Some Random Thoughts</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../random-thoughts.html#pan-intelligence">Pan-Intelligence</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../random-thoughts.html#why-can-we-understand-the-universe">Why Can We Understand the Universe?</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../ref.html">References</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../acknowledgement.html">Acknowledgement</a></li>
</ul>
</ul>
</li>
              
                <li class="dropdown">
  <a role="button"
     id="dLabelLocalToc"
     data-toggle="dropdown"
     data-target="#"
     href="#">Sections <b class="caret"></b></a>
  <ul class="dropdown-menu localtoc"
      role="menu"
      aria-labelledby="dLabelLocalToc"><ul>
<li><a class="reference internal" href="#">Introductions (Review) and Several Preliminary Statistical Methods</a><ul>
<li><a class="reference internal" href="#review">Review</a></li>
<li><a class="reference internal" href="#least-squares-and-nearest-neighbors">Least Squares and Nearest Neighbors</a><ul>
<li><a class="reference internal" href="#least-squares">Least Squares</a></li>
<li><a class="reference internal" href="#nearest-neighbor">Nearest-Neighbor</a></li>
<li><a class="reference internal" href="#for-which-scenario">For Which Scenario</a></li>
</ul>
</li>
<li><a class="reference internal" href="#statistical-decision-theory">Statistical Decision Theory</a><ul>
<li><a class="reference internal" href="#id1">Nearest-Neighbor</a></li>
</ul>
</li>
<li><a class="reference internal" href="#local-methods-in-high-dimensions">Local Methods in High Dimensions</a></li>
<li><a class="reference internal" href="#statistical-models-supervised-learning-and-function-approximation">Statistical Models, Supervised Learning and Function Approximation</a><ul>
<li><a class="reference internal" href="#joint-distribution">Joint Distribution</a></li>
<li><a class="reference internal" href="#supervised-learning">Supervised Learning</a></li>
<li><a class="reference internal" href="#function-approximation">Function Approximation</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</ul>
</li>
              
            
            
              
                
  <li>
    <a href="index.html" title="Previous Chapter: The Element of Statistical Learning"><span class="glyphicon glyphicon-chevron-left visible-sm"></span><span class="hidden-sm hidden-tablet">&laquo; The Element o...</span>
    </a>
  </li>
  <li>
    <a href="linear-methods-for-regresssion.html" title="Next Chapter: Linear Methods for Regression"><span class="glyphicon glyphicon-chevron-right visible-sm"></span><span class="hidden-sm hidden-tablet">Linear Method... &raquo;</span>
    </a>
  </li>
              
            
            
            
            
          </ul>

          
            
<form class="navbar-form navbar-right" action="../../search.html" method="get">
 <div class="form-group">
  <input type="text" name="q" class="form-control" placeholder="Search" />
 </div>
  <input type="hidden" name="check_keywords" value="yes" />
  <input type="hidden" name="area" value="default" />
</form>
          
        </div>
    </div>
  </div>

<div class="container">
  <div class="row">
    <div class="col-md-12 content">
      
  <div class="section" id="introductions-review-and-several-preliminary-statistical-methods">
<h1>Introductions (Review) and Several Preliminary Statistical Methods<a class="headerlink" href="#introductions-review-and-several-preliminary-statistical-methods" title="Permalink to this headline">¶</a></h1>
<div class="note admonition">
<p class="first admonition-title">Summary</p>
<p class="last">Some basics of statistical learning; least squares and k nearest neighbors; statistical decision theory; local methods in high dimensions</p>
</div>
<div class="section" id="review">
<h2>Review<a class="headerlink" href="#review" title="Permalink to this headline">¶</a></h2>
<p><strong>Skeleton notes</strong></p>
<div class="note admonition">
<p class="first admonition-title">Abbreviations</p>
<ol class="last arabic simple">
<li>MSE: mean squared error</li>
<li>EPE: expected prediction error</li>
<li>RSS: sum of squares</li>
</ol>
</div>
<div class="note admonition">
<p class="first admonition-title">Notations</p>
<p>Fonts:</p>
<ol class="arabic simple">
<li>Vectors or scalars are denoted by italic math font <img class="math" src="../../_images/math/7a7bb470119808e2db2879fc2b2526f467b7a40b.png" alt="X"/>.</li>
<li>Components of vectors are denoted by subscripts <img class="math" src="../../_images/math/9a63346d0b18c3361d2ba446d7c9e7cf831d8c4d.png" alt="X_i"/>.</li>
<li>Matrix is denoted by math bold font <img class="math" src="../../_images/math/ddd65d14f26b65e448158127e47b2409d491a4e5.png" alt="\mathbf X"/>.</li>
</ol>
<p>Symbols</p>
<ol class="last arabic simple">
<li><img class="math" src="../../_images/math/7a7bb470119808e2db2879fc2b2526f467b7a40b.png" alt="X"/> for input variables;</li>
<li><img class="math" src="../../_images/math/0062c26909b3e07ee8f5a6285b2563d69bc979ff.png" alt="Y"/> for quantitative output;</li>
<li><img class="math" src="../../_images/math/dee63237674afc3154c2e33b7b68d67f85f2ee0a.png" alt="G"/> for qualitative output;</li>
<li><img class="math" src="../../_images/math/ea3434cc91d7aabf9ed0f5b476f25a6fcc5690e4.png" alt="\hat {}"/> for prediction.</li>
</ol>
</div>
</div>
<div class="section" id="least-squares-and-nearest-neighbors">
<h2>Least Squares and Nearest Neighbors<a class="headerlink" href="#least-squares-and-nearest-neighbors" title="Permalink to this headline">¶</a></h2>
<div class="section" id="least-squares">
<h3>Least Squares<a class="headerlink" href="#least-squares" title="Permalink to this headline">¶</a></h3>
<p>Least square model:</p>
<div class="math">
<p><img src="../../_images/math/362d989cabfd7fe406857f062f6865bb3678b27b.png" alt="\hat Y = X^T \hat \beta."/></p>
</div><p>Residual sum of squares (RSS):</p>
<div class="math">
<p><img src="../../_images/math/7d7231321ca351f9239342a9135ca6eb301151ac.png" alt="\mathrm{RSS}(\beta) = (\mathbf y - \mathbf X \beta)^{\mathrm T} (\mathbf y - \mathbf X \beta)."/></p>
</div><p>The parameters we need is the set that minimizes RSS, which requires</p>
<div class="math">
<p><img src="../../_images/math/311da1778adff526a209178656b00f7f6eb66ee3.png" alt="\frac{d}{d\beta} \mathrm{RSS} = 0."/></p>
</div><p>So we can solve the parameters easily.</p>
</div>
<div class="section" id="nearest-neighbor">
<h3>Nearest-Neighbor<a class="headerlink" href="#nearest-neighbor" title="Permalink to this headline">¶</a></h3>
<ol class="arabic">
<li><p class="first">For input data <img class="math" src="../../_images/math/a59f68a4202623bb859a7093f0316bf466e6f75d.png" alt="x"/>, calculate the Euclidean distance between <img class="math" src="../../_images/math/a59f68a4202623bb859a7093f0316bf466e6f75d.png" alt="x"/> and other input data <img class="math" src="../../_images/math/bdb2d04d69b82c2288f5ef46664d548355e130af.png" alt="x_j"/>.</p>
</li>
<li><p class="first">Choose the <img class="math" src="../../_images/math/0b7c1e16a3a8a849bb8ffdcdbf86f65fd1f30438.png" alt="k"/> nearest neighbors based on the distance.</p>
</li>
<li><p class="first">Output prediction is determined by average of the corresponding outputs of the selected inputs.</p>
<div class="math">
<p><img src="../../_images/math/eab188e21d8ea2794b6e15d93d17f3cbe02cf3f9.png" alt="\hat Y = \frac{1}{k} \sum_{N_k} y_i."/></p>
</div></li>
</ol>
<div class="note admonition">
<p class="first admonition-title">Metric of Distance</p>
<p class="last">For the calculation of distance, metric must be implemented. The book used examples of Euclidean metric. Another metric that can be inspiring is the hyperbolic space. I talked about this in <a class="reference internal" href="../../network/popularity-similarity.html#popularitysimilarity"><span class="std std-ref">Popularity vs Similarity in Growing Networks</span></a>.</p>
</div>
</div>
<div class="section" id="for-which-scenario">
<h3>For Which Scenario<a class="headerlink" href="#for-which-scenario" title="Permalink to this headline">¶</a></h3>
<ol class="arabic simple">
<li>Least squares: Gaussian-like data set;</li>
<li>Nearest-Neighbor: mixture of Gaussians.</li>
</ol>
<div class="warning admonition">
<p class="first admonition-title">Mixture of Gaussian</p>
<p class="last">Mixture of Gaussians can be described by generative model. I am not really sure what that is. It seems to me that the final data is basically generated from Gaussians of different parameters which are generated randomly.</p>
</div>
</div>
</div>
<div class="section" id="statistical-decision-theory">
<h2>Statistical Decision Theory<a class="headerlink" href="#statistical-decision-theory" title="Permalink to this headline">¶</a></h2>
<ol class="arabic">
<li><p class="first">Given input <img class="math" src="../../_images/math/7a7bb470119808e2db2879fc2b2526f467b7a40b.png" alt="X"/> and output <img class="math" src="../../_images/math/0062c26909b3e07ee8f5a6285b2563d69bc979ff.png" alt="Y"/>;</p>
</li>
<li><p class="first">Following a joint distribution <img class="math" src="../../_images/math/c39abefda906a5553a550e771f07dd3e9fe16f8c.png" alt="\mathrm{Pr}(X,Y)"/>;</p>
</li>
<li><p class="first">Based on input and output, we look for a function that predicts the behavior, i.e., <img class="math" src="../../_images/math/0f80e9e23ce74b43d16640ea99db0ddb3356945c.png" alt="\hat Y = f(X)"/>;</p>
</li>
<li><p class="first">How well the prediction is defined by squared error loss <img class="math" src="../../_images/math/2f014da4ffeae990d6bed1187a5805bd3a89d32b.png" alt="L(Y,\hat Y) = (Y-\hat Y)^2"/>.</p>
</li>
<li><p class="first">With the distribution, we predict the expected prediction error (EPE) as</p>
<div class="math">
<p><img src="../../_images/math/42a439d37ed125e32ba0ac595849276e30337db6.png" alt="\mathrm{EPE}(f) = E[ ( Y- \hat Y )^2 ] = \int (y - f(x))^2 \mathrm{Pr}(dx, dy)."/></p>
</div></li>
<li><p class="first">The book derived that the best prediction is <img class="math" src="../../_images/math/8beb3fc759ce4e263e85e7d39afb954b336880d2.png" alt="f(x) = E(Y\vert X=x)"/>.</p>
</li>
<li><p class="first">Different loss functions lead to different EPE’s.</p>
</li>
</ol>
<div class="warning admonition">
<p class="first admonition-title">About Probability Distribution</p>
<p class="last">Question: Can we simply solve the probability distribution and find out the function of prediction? The conclusion says the best prediction of <img class="math" src="../../_images/math/0062c26909b3e07ee8f5a6285b2563d69bc979ff.png" alt="Y"/> is the conditional mean. Is it effectively solving <img class="math" src="../../_images/math/0062c26909b3e07ee8f5a6285b2563d69bc979ff.png" alt="Y"/> from the probability distribution?</p>
</div>
<div class="section" id="id1">
<h3>Nearest-Neighbor<a class="headerlink" href="#id1" title="Permalink to this headline">¶</a></h3>
<ol class="arabic simple">
<li>The best prediction based on EPE is conditional mean, Eq. 2.13;</li>
<li>Both <img class="math" src="../../_images/math/0b7c1e16a3a8a849bb8ffdcdbf86f65fd1f30438.png" alt="k"/> nearest neighbor and linear regression fits into this framework;</li>
<li>Additive models: basically turn the linear <img class="math" src="../../_images/math/26dbef056fdf587be186e462819f66c7af81f62e.png" alt="x^T\beta"/> into a function of <img class="math" src="../../_images/math/e9cb930e80b02047712974833701afe482e7e628.png" alt="f_j(X_j)"/>. The summation still holds.</li>
<li>The best prediction based on expectation only is conditional median.</li>
<li>Categorical variable <img class="math" src="../../_images/math/dee63237674afc3154c2e33b7b68d67f85f2ee0a.png" alt="G"/> also follows the same paradigm but with different loss function.</li>
<li>A choice of loss function for categorical case is a matrix. It has to be a matrix because we have to specify penalties a given prediction class compared to the output class. The dimension of this matrix should be the number of categories. It is rank 2.</li>
</ol>
<div class="note admonition">
<p class="first admonition-title">Some comments on this section</p>
<ol class="last arabic simple">
<li>0 neighbor indicates an exact classification for the sample data but without the implementation of expectation values at each point since there is only one value at that point in one set of sample data;</li>
<li><img class="math" src="../../_images/math/0b7c1e16a3a8a849bb8ffdcdbf86f65fd1f30438.png" alt="k"/> nearest neighbor assumed that expectation around a small patch of a point is identical to expectation at the exact point with the corresponding distribution.</li>
<li>In <strong>Monte Carlo method</strong>, calculation of volume in high dimension converges very slowly. The reason is that we need a very large number of sampling points since the dimension is high. The procedure is multiplicative. The same thing might happen here. <img class="math" src="../../_images/math/0b7c1e16a3a8a849bb8ffdcdbf86f65fd1f30438.png" alt="k"/> nearest neighbor is basically some kind of averaging procedure of the volume density. It requires a large number of sample data points to perform an fairly accurate average.</li>
<li>The linear regression is basically a first order Taylor expansion of the approximator <img class="math" src="../../_images/math/eda52292f6952f7e27fef52e7ce8393981770d2c.png" alt="f(x)"/>, <img class="math" src="../../_images/math/2a32141136dad55067dbffea358b1e367a2ea028.png" alt="f(x) = x^T\beta"/>.</li>
</ol>
</div>
</div>
</div>
<div class="section" id="local-methods-in-high-dimensions">
<h2>Local Methods in High Dimensions<a class="headerlink" href="#local-methods-in-high-dimensions" title="Permalink to this headline">¶</a></h2>
<ol class="arabic">
<li><p class="first">Curse of high dimensions: edge length of a cube of volume <img class="math" src="../../_images/math/eaa6ad49a7f78fe5a13b486690163bf2dc7e3e60.png" alt="r"/> is <img class="math" src="../../_images/math/e3c51e7df72e2878cb80a8f41717e41ab948af43.png" alt="e_p(r) = r^{1/p}"/>. An extreme example: <img class="math" src="../../_images/math/ce8322ffec9b12a50c61cb40b88f4c5382b956f8.png" alt="(10^{-10})^{1/10} =0.1"/>.</p>
</li>
<li><p class="first">Small volume leads to high variance.</p>
</li>
<li><p class="first">Homogeneous sampling doesn’t work in high dimensions. Since most points will fall near the edges.</p>
<div class="figure align-center" id="id2">
<img alt="../../_images/10dsphere-volume-vs-radius.png" src="../../_images/10dsphere-volume-vs-radius.png" />
<p class="caption"><span class="caption-text">Volume of 10D sphere as a function of radius.</span></p>
</div>
</li>
<li><p class="first">Requires huge number of sample points in high dimensions.</p>
</li>
</ol>
</div>
<div class="section" id="statistical-models-supervised-learning-and-function-approximation">
<h2>Statistical Models, Supervised Learning and Function Approximation<a class="headerlink" href="#statistical-models-supervised-learning-and-function-approximation" title="Permalink to this headline">¶</a></h2>
<div class="section" id="joint-distribution">
<h3>Joint Distribution<a class="headerlink" href="#joint-distribution" title="Permalink to this headline">¶</a></h3>
<div class="note admonition">
<p class="first admonition-title">Weird SubSection</p>
<p class="last">I didn’t not get the point of this subsection. It seems that the authors are talking about whether it is proper to assume the relation between input and output is deterministic.</p>
</div>
</div>
<div class="section" id="supervised-learning">
<h3>Supervised Learning<a class="headerlink" href="#supervised-learning" title="Permalink to this headline">¶</a></h3>
<ol class="arabic simple">
<li>learn by example.</li>
</ol>
</div>
<div class="section" id="function-approximation">
<h3>Function Approximation<a class="headerlink" href="#function-approximation" title="Permalink to this headline">¶</a></h3>
<ol class="arabic simple">
<li>Linear model;</li>
<li>Function as basis (Eq. 2.30): <img class="math" src="../../_images/math/bf53149e9e63b8e2ff3b11df9c046786de148b6d.png" alt="f_\theta(x) = \sum h_k(x)\theta_k"/>.</li>
<li>Examples of function bases are Fourier expansions, sigmoid, etc.</li>
<li>Learning through minimizing sum of squares (RSS), or maximum likelihood estimation, etc.</li>
<li>Maximum likelihood estimation:
1. Likelihood: <img class="math" src="../../_images/math/65ada0ff59e346c52ba1976e51ee99715451c2d5.png" alt="L(\theta) = \sum_{i=1}^N \log \mathrm{Pr}_\theta (y_i)"/>;
2. Maximized it (“probability of the observed sample is largest”)
3. Minimizing RSS is equivalent to maximum likelihood estimation. Eq. 2.35.</li>
</ol>
</div>
</div>
</div>


    </div>
      
  </div>
</div>

<!-- <div class="container" style="border-top:solid 1px black;margin-top:10px;padding-top:10px;padding-bottom:10px;">
<div id="disqus_thread"></div>
    <script type="text/javascript">
        /* * * CONFIGURATION VARIABLES: EDIT BEFORE PASTING INTO YOUR WEBPAGE * * */
        var disqus_shortname = 'statmech'; // required: replace example with your forum shortname

        /* * * DON'T EDIT BELOW THIS LINE * * */
        (function() {
            var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
            dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
            (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
        })();
    </script>
    <noscript>Please enable JavaScript to view the <a href="http://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
    <a href="http://disqus.com" class="dsq-brlink">comments powered by <span class="logo-disqus">Disqus</span></a>

</div> -->

<hr>


<div class="container" style="border-top:solid 1px black;margin-top:10px;padding-top:10px;padding-bottom:10px;">
    <p class="pull-right">
        <a href="#"><span class="glyphicon glyphicon-chevron-up" aria-hidden="true"></span></a>

    </p>
    <p>
        <span>© 2018, Lei Ma</span>|
        <a href="https://github.com/emptymalei/intelligence">GitHub</a>|
        <span><span class="glyphicon glyphicon-book" aria-hidden="true"></span>
              <a href="http://statisticalphysics.openmetric.org">Statistical Mechanics Notebook</a>
        </span> |
        <span><span class="glyphicon glyphicon-list" aria-hidden="true"></span>
        <a href="/genindex.html">Index</a>
     </span> |
       <a href="../../_sources/statistics/esl/statistical-learning-theory.rst.txt"
           rel="nofollow">Page Source</a></li>|
       <a href="/changelog.html"
           rel="nofollow">changelog</a></li>|
        <span>Created with
            <a href="http://www.sphinx-doc.org">Sphinx</a></span>
    </p>

    <hr>


    <script>
        window.jQuery || document.write('<script src="_static/js/vendor/jquery-1.9.1.min.js"><\/script>')
    </script>

</div>


<script type="text/javascript">
    var _gaq = _gaq || [];
    _gaq.push(['_setAccount', 'UA-44466929-2']);
    _gaq.push(['_trackPageview']);

    (function() {
        var ga = document.createElement('script');
        ga.type = 'text/javascript';
        ga.async = true;
        ga.src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';
        var s = document.getElementsByTagName('script')[0];
        s.parentNode.insertBefore(ga, s);
    })();
</script>


  </body>
</html>