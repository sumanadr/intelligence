%% Generated by Sphinx.
\def\sphinxdocclass{report}
\documentclass[letterpaper,10pt,english]{sphinxmanual}
\ifdefined\pdfpxdimen
   \let\sphinxpxdimen\pdfpxdimen\else\newdimen\sphinxpxdimen
\fi \sphinxpxdimen=.75bp\relax

\usepackage[utf8]{inputenc}
\ifdefined\DeclareUnicodeCharacter
 \ifdefined\DeclareUnicodeCharacterAsOptional
  \DeclareUnicodeCharacter{"00A0}{\nobreakspace}
  \DeclareUnicodeCharacter{"2500}{\sphinxunichar{2500}}
  \DeclareUnicodeCharacter{"2502}{\sphinxunichar{2502}}
  \DeclareUnicodeCharacter{"2514}{\sphinxunichar{2514}}
  \DeclareUnicodeCharacter{"251C}{\sphinxunichar{251C}}
  \DeclareUnicodeCharacter{"2572}{\textbackslash}
 \else
  \DeclareUnicodeCharacter{00A0}{\nobreakspace}
  \DeclareUnicodeCharacter{2500}{\sphinxunichar{2500}}
  \DeclareUnicodeCharacter{2502}{\sphinxunichar{2502}}
  \DeclareUnicodeCharacter{2514}{\sphinxunichar{2514}}
  \DeclareUnicodeCharacter{251C}{\sphinxunichar{251C}}
  \DeclareUnicodeCharacter{2572}{\textbackslash}
 \fi
\fi
\usepackage{cmap}
\usepackage[T1]{fontenc}
\usepackage{amsmath,amssymb,amstext}
\usepackage{babel}
\usepackage{times}
\usepackage[Bjarne]{fncychap}
\usepackage[dontkeepoldnames]{sphinx}

\usepackage{geometry}

% Include hyperref last.
\usepackage{hyperref}
% Fix anchor placement for figures with captions.
\usepackage{hypcap}% it must be loaded after hyperref.
% Set up styles of URL: it should be placed after hyperref.
\urlstyle{same}

\addto\captionsenglish{\renewcommand{\figurename}{Fig.}}
\addto\captionsenglish{\renewcommand{\tablename}{Table}}
\addto\captionsenglish{\renewcommand{\literalblockname}{Listing}}

\addto\captionsenglish{\renewcommand{\literalblockcontinuedname}{continued from previous page}}
\addto\captionsenglish{\renewcommand{\literalblockcontinuesname}{continues on next page}}

\addto\extrasenglish{\def\pageautorefname{page}}

\setcounter{tocdepth}{1}


\usepackage{dsfont}
\usepackage{slashed}
\usepackage{yfonts}
\usepackage{mathrsfs}
\def\degrees{^\circ}
\def\d{{\rm d}}

\usepackage{color}
\setcounter{tocdepth}{2}

\def\sign{\mathop{\mathrm{sign}}}
\def\L{{\mathcal L}}
\def\H{{\mathcal H}}
\def\M{{\mathcal M}}
\def\matrix{}
\def\fslash#1{#1 \!\!\!/}
\def\F{{\bf F}}
\def\R{{\bf R}}
\def\J{{\bf J}}
\def\x{{\bf x}}
\def\y{{\bf y}}
\def\h{{\rm h}}
\def\a{{\rm a}}
\newcommand{\bfx}{\mbox{\boldmath $x$}}
\newcommand{\bfy}{\mbox{\boldmath $y$}}
\newcommand{\bfz}{\mbox{\boldmath $z$}}
\newcommand{\bfv}{\mbox{\boldmath $v$}}
\newcommand{\bfu}{\mbox{\boldmath $u$}}
\newcommand{\bfF}{\mbox{\boldmath $F$}}
\newcommand{\bfJ}{\mbox{\boldmath $J$}}
\newcommand{\bfU}{\mbox{\boldmath $U$}}
\newcommand{\bfY}{\mbox{\boldmath $Y$}}
\newcommand{\bfR}{\mbox{\boldmath $R$}}
\newcommand{\bfg}{\mbox{\boldmath $g$}}
\newcommand{\bfc}{\mbox{\boldmath $c$}}
\newcommand{\bfxi}{\mbox{\boldmath $\xi$}}

\newcommand{\bra}[1]{\left\langle #1\right|}
\newcommand{\ket}[1]{\left| #1\right\rangle}
\newcommand{\braket}[2]{\langle #1 \mid #2 \rangle}
\newcommand{\avg}[1]{\left< #1 \right>}

%\def\back{\!\!\!\!\!\!\!\!\!\!}
\def\back{}
\def\col#1#2{\left(\matrix{#1#2}\right)}
\def\row#1#2{\left(\matrix{#1#2}\right)}
\def\mat#1{\begin{pmatrix}#1\end{pmatrix}}
\def\matd#1#2{\left(\matrix{#1\back0\cr0\back#2}\right)}
\def\p#1#2{{\partial#1\over\partial#2}}
\def\cg#1#2#3#4#5#6{({#1},\,{#2},\,{#3},\,{#4}\,|\,{#5},\,{#6})}
\def\half{{\textstyle{1\over2}}}
\def\jsym#1#2#3#4#5#6{\left\{\matrix{
{#1}{#2}{#3}
{#4}{#5}{#6}
}\right\}}
\def\diag{\hbox{diag}}

\font\dsrom=dsrom10
\def\one{\hbox{\dsrom 1}}

\def\res{\mathop{\mathrm{Res}}}

\def\mathnot#1{\text{"$#1$"}}


%See Character Table for cmmib10:
%http://www.math.union.edu/~dpvc/jsmath/download/extra-fonts/cmmib10/cmmib10.html
\font\mib=cmmib10
\def\balpha{\hbox{\mib\char"0B}}
\def\bbeta{\hbox{\mib\char"0C}}
\def\bgamma{\hbox{\mib\char"0D}}
\def\bdelta{\hbox{\mib\char"0E}}
\def\bepsilon{\hbox{\mib\char"0F}}
\def\bzeta{\hbox{\mib\char"10}}
\def\boldeta{\hbox{\mib\char"11}}
\def\btheta{\hbox{\mib\char"12}}
\def\biota{\hbox{\mib\char"13}}
\def\bkappa{\hbox{\mib\char"14}}
\def\blambda{\hbox{\mib\char"15}}
\def\bmu{\hbox{\mib\char"16}}
\def\bnu{\hbox{\mib\char"17}}
\def\bxi{\hbox{\mib\char"18}}
\def\bpi{\hbox{\mib\char"19}}
\def\brho{\hbox{\mib\char"1A}}
\def\bsigma{\hbox{\mib\char"1B}}
\def\btau{\hbox{\mib\char"1C}}
\def\bupsilon{\hbox{\mib\char"1D}}
\def\bphi{\hbox{\mib\char"1E}}
\def\bchi{\hbox{\mib\char"1F}}
\def\bpsi{\hbox{\mib\char"20}}
\def\bomega{\hbox{\mib\char"21}}

\def\bvarepsilon{\hbox{\mib\char"22}}
\def\bvartheta{\hbox{\mib\char"23}}
\def\bvarpi{\hbox{\mib\char"24}}
\def\bvarrho{\hbox{\mib\char"25}}
\def\bvarphi{\hbox{\mib\char"27}}

%how to use:
%$$\alpha\balpha$$
%$$\beta\bbeta$$
%$$\gamma\bgamma$$
%$$\delta\bdelta$$
%$$\epsilon\bepsilon$$
%$$\zeta\bzeta$$
%$$\eta\boldeta$$
%$$\theta\btheta$$
%$$\iota\biota$$
%$$\kappa\bkappa$$
%$$\lambda\blambda$$
%$$\mu\bmu$$
%$$\nu\bnu$$
%$$\xi\bxi$$
%$$\pi\bpi$$
%$$\rho\brho$$
%$$\sigma\bsigma$$
%$$\tau\btau$$
%$$\upsilon\bupsilon$$
%$$\phi\bphi$$
%$$\chi\bchi$$
%$$\psi\bpsi$$
%$$\omega\bomega$$
%
%$$\varepsilon\bvarepsilon$$
%$$\vartheta\bvartheta$$
%$$\varpi\bvarpi$$
%$$\varrho\bvarrho$$
%$$\varphi\bvarphi$$

%small font
\font\mibsmall=cmmib7
\def\bsigmasmall{\hbox{\mibsmall\char"1B}}

\def\Tr{\hbox{Tr}\,}
\def\Arg{\hbox{Arg}}
\def\atan{\hbox{atan}}


\title{Intelligence}
\date{Oct 29, 2017}
\release{0.0.6}
\author{OctoMiao}
\newcommand{\sphinxlogo}{\vbox{}}
\renewcommand{\releasename}{Release}
\makeindex

\begin{document}

\maketitle
\sphinxtableofcontents
\phantomsection\label{\detokenize{index::doc}}


There is a deeply hidden secret, one that killed many, one that saved many. Intelligence, the holy grail of human society, is the ultimate science of all time. As I think about intelligence of many forms, I feel the urge to learn more about them. Hence the notes.

A particular interest is to find resonances in these intelligence. A chinese proverb says, little force can move a huge rock (四两拨千斤). Resonances are one of the maneuvers to deflect a huge rock.


\chapter{Vocabulary}
\label{\detokenize{vocabulary::doc}}\label{\detokenize{vocabulary:intelligence}}\label{\detokenize{vocabulary:vocabulary}}

\chapter{Questions? Yes}
\label{\detokenize{questions:questions-yes}}\label{\detokenize{questions::doc}}

\chapter{Neuroscience}
\label{\detokenize{neuroscience/index::doc}}\label{\detokenize{neuroscience/index:neuroscience}}

\section{Biology}
\label{\detokenize{neuroscience/biology::doc}}\label{\detokenize{neuroscience/biology:biology}}

\subsection{Terms \& Concepts}
\label{\detokenize{neuroscience/biology:terms-concepts}}\begin{enumerate}
\item {} \begin{description}
\item[{cell structure (check figure)}] \leavevmode\begin{enumerate}
\item {} 
Nucleus: administration office

\item {} 
smooth endoplasmic reticulum: high way

\item {} 
rough endoplasmic reticulum

\item {} 
Mitochondrion: power plant

\item {} 
Cytoplasm

\item {} 
Centriole: combines the surrounding structure to be the centrosome

\item {} 
Lysosome: trash can, and recycle system

\item {} 
plasma membrane

\item {} 
Golgi complex: post office

\end{enumerate}

\end{description}

\item {} \begin{description}
\item[{Secretory}] \leavevmode\begin{enumerate}
\item {} 
Produced  at RER

\item {} 
Wrapped at Golgi

\end{enumerate}

\end{description}

\item {} \begin{description}
\item[{Cell division cycle: For neurons, G0 is when the cells decides to divide or differentiate}] \leavevmode\begin{enumerate}
\item {} \begin{description}
\item[{Interphase}] \leavevmode\begin{enumerate}
\item {} 
G0-\textgreater{}G1-\textgreater{}S-\textgreater{}G2

\end{enumerate}

\end{description}

\item {} 
Mitosis

\end{enumerate}

\end{description}

\item {} \begin{description}
\item[{Cell membrane}] \leavevmode\begin{enumerate}
\item {} \begin{description}
\item[{G Protein Coupled Receptors (GPCR): 7 transmembrane}] \leavevmode\begin{enumerate}
\item {} 
G proteins have three subunits: alpha beta gamma, which are inactive if no GPCR’s are in action

\item {} 
Receptors for G Protein and important for signaling pathway: ligand binds to GPCR -\textgreater{} GPCR conformational change -\textgreater{} alpha sub-unit exchange its binded GDP for a GTP (now it binds a GTP, and both alpha subunit and beta-gamma complex are active) -\textgreater{} regulate target proteins which are responsible for second messengers -\textgreater{} GTP on alpba subunit then hydrolyze to GDP -\textgreater{} everything comes back together to the original state

\end{enumerate}

\end{description}

\end{enumerate}

\end{description}

\item {} 
Protein phosphatase: removes phosphate group from phosphorylated amino acid.

\item {} 
Phosphorylation: transfer phosphate group to enzymes

\item {} 
Kinase: transfer phosphate group from high-energy to another place

\item {} 
Tyrosine kinase: transfer phosphate group to a protein

\item {} 
Homeobox: a dna sequence that regulates anatomical development

\end{enumerate}


\subsection{Techniques}
\label{\detokenize{neuroscience/biology:techniques}}\begin{enumerate}
\item {} \begin{description}
\item[{Western blot: separate protein by length of polypeptides}] \leavevmode\begin{enumerate}
\item {} 
Detergent -\textgreater{} gel electrophoresis -\textgreater{} transfer \& incubation

\end{enumerate}

\end{description}

\item {} 
Northern blot: RNA

\item {} \begin{description}
\item[{Southern blot: DNA}] \leavevmode\begin{enumerate}
\item {} 
DNA fragments-\textgreater{}gel electrophoresis-\textgreater{}ultraviolet

\end{enumerate}

\end{description}

\item {} \begin{description}
\item[{PCR}] \leavevmode\begin{enumerate}
\item {} 
DNA fragmentation-\textgreater{}PCR machine-\textgreater{}gel electrophoresis-\textgreater{}ultraviolet

\end{enumerate}

\end{description}

\item {} 
Clone: somatic cell nucleus + zygote (without nucleus)

\end{enumerate}


\subsection{Receptors and ligands}
\label{\detokenize{neuroscience/biology:receptors-and-ligands}}\begin{enumerate}
\item {} \begin{description}
\item[{Agonist}] \leavevmode\begin{enumerate}
\item {} 
Has the same effect as a neurotransmitter

\end{enumerate}

\end{description}

\item {} \begin{description}
\item[{Antagonist}] \leavevmode\begin{enumerate}
\item {} 
Doesn’t really work like neurotransmitter but binds to receptor

\end{enumerate}

\end{description}

\end{enumerate}


\subsection{Neurodevelopment}
\label{\detokenize{neuroscience/biology:neurodevelopment}}\begin{enumerate}
\item {} 
Process:  Fertilized egg -\textgreater{} caps -\textgreater{} gastrulation-\textgreater{} neural plate-\textgreater{}neural tube-\textgreater{} Anterior-posterior patterning (now we have differences between two ends)-\textgreater{} Dorsal-ventral patterning (differences between dorsal and ventral)

\item {} 
Gastrulation: three layers : ectoderm, endoderm, then mesoderm

\item {} \begin{description}
\item[{Polarization of Neurons:}] \leavevmode\begin{enumerate}
\item {} 
5 stages: first 2 symmetric, 3 with axon begins, 4 growing in length and branching, 5 morphogenesis  and synapse

\item {} 
Filopodia:

\item {} 
Lamellipodia:

\item {} 
Framework: tublin + actin

\end{enumerate}

\end{description}

\item {} \begin{description}
\item[{ECM: extracellular matrix:}] \leavevmode\begin{enumerate}
\item {} 
Scaffolding the organ

\end{enumerate}

\end{description}

\end{enumerate}


\subsection{Refs \& Notes}
\label{\detokenize{neuroscience/biology:refs-notes}}

\section{Visual Cortex}
\label{\detokenize{neuroscience/visual-cortex::doc}}\label{\detokenize{neuroscience/visual-cortex:visual-cortex}}

\subsection{FS in Visual Cortex}
\label{\detokenize{neuroscience/visual-cortex:fs-in-visual-cortex}}
\begin{sphinxadmonition}{note}{Keywords}
\begin{enumerate}
\item {} 
Orientation selective neurons.

\item {} 
Feedforward

\item {} 
Recurrent

\end{enumerate}

Experiments:
\begin{enumerate}
\item {} 
Linear amplifications of thalamic input

\item {} 
Emergence of pairwise correlations

\item {} 
Link between spontaneous and evoked activity. ( {\hyperref[\detokenize{neuroscience/misc:resting-state-networks}]{\sphinxcrossref{\DUrole{std,std-ref}{RSN (Resting State Networks)}}}} )

\end{enumerate}

Theory:
\begin{enumerate}
\item {} 
Orientation Selectivity Index: OSI

\end{enumerate}
\end{sphinxadmonition}
\begin{enumerate}
\item {} 
Arise of feature-specific (FS) connectivity: induced by learning rule?

\end{enumerate}


\section{Memory}
\label{\detokenize{neuroscience/memory/index::doc}}\label{\detokenize{neuroscience/memory/index:memory}}

\subsection{Time Scale in Memory}
\label{\detokenize{neuroscience/memory/time-scale::doc}}\label{\detokenize{neuroscience/memory/time-scale:time-scale-in-memory}}

\subsubsection{Learning and Memory}
\label{\detokenize{neuroscience/memory/time-scale:learning-and-memory}}\begin{enumerate}
\item {} 
Three categories of memory: \phantomsection\label{\detokenize{neuroscience/memory/time-scale:id1}}{\hyperref[\detokenize{neuroscience/memory/time-scale:tetzlaff2012}]{\sphinxcrossref{{[}Tetzlaff2012{]}}}}
\begin{enumerate}
\item {} 
Working memory: processing information and usually requires with atention.

\item {} 
Short-term memory

\item {} 
Long-term memory

\end{enumerate}

\item {} 
Evidence of plasticity (Martin et al 2000):
\begin{enumerate}
\item {} 
Detectability: measured change in synapse when animals is learning or momorizing.

\item {} 
Mimicry: replicate the synapse change from one animal to another would result in similar memory or behavior. Not yet experimentally realized.

\item {} 
Anterograde alteration: disable synaptic plasticity and learning and memory should stop working.

\item {} 
Retrograde alteration: modify synapse weight and the memory should be changed.

\end{enumerate}

\end{enumerate}

\begin{sphinxadmonition}{note}{Why Do We Care about Time Scales}

Memory works on many different time scales. This is extremely important for life forms because they need to work out correlations between events in the past and current status.

I would think of permenant memory would serve the purpose of finding out the casalities. However it won’t be optimized to work on small time scales. The suitation might be quite similar to computers.
\end{sphinxadmonition}


\subsubsection{Memory Consolidation and Synapse Scaling}
\label{\detokenize{neuroscience/memory/time-scale:memory-consolidation-and-synapse-scaling}}
Long-term memory requires memory consolidation, which consolidates the synapse locally to enhence memory or transfers momories to other area.

At the region that connects hippocampus and cortex, LTS (Long-term storage) and STS (Short-term storage) happens at the same time. This indicates that neurons in this cross-section mentains two different types of dynamics, one of which responds to consolidation (i.e., synapse weight change), while the other doesn’t.

\begin{sphinxadmonition}{note}{\#TIL\# Recall momory can destablize the corresponding memory itself}

In the paper by Tetzlaff \phantomsection\label{\detokenize{neuroscience/memory/time-scale:id2}}{\hyperref[\detokenize{neuroscience/memory/time-scale:tetzlaff2013}]{\sphinxcrossref{{[}Tetzlaff2013{]}}}}, they mentioned it.
\end{sphinxadmonition}

Tetzlaff et al found a solution to this time scale problem \phantomsection\label{\detokenize{neuroscience/memory/time-scale:id3}}{\hyperref[\detokenize{neuroscience/memory/time-scale:tetzlaff2013}]{\sphinxcrossref{{[}Tetzlaff2013{]}}}}. In principle, \(\frac{d w^{+}_{ij}}{dt}\) can be Taylor expanded to have all orders of \(w^{+}_{ij}\). In this work by Tetzlaff, they included a simple plasticity (zeroth order in Taylor expansion)
\begin{equation*}
\begin{split}\mu F_{i} F_{j},\end{split}
\end{equation*}
as well as a synapse scaling (second order in Taylor expansion)
\begin{equation*}
\begin{split}\frac{\mu}{\kappa} (F^T - F_i) \left( w^{+}_{ij} \right)^2.\end{split}
\end{equation*}
We put those two together,
\begin{equation*}
\begin{split}\frac{d w^{+}_{ij}}{dt} = \mu \left( F_{i} F_{j} +  \kappa (F^T - F_i) \left( w^{+}_{ij} \right)^2 \right).\end{split}
\end{equation*}
Such a updating rule for weights contains dynamics of bifurcation. For a region of input frequencies, activity and weights exhibit bistability, thus leading to neurons evolve to different states. Neurons recieved strong inputs have larger weights due to plasticity. Those neurons with large weights evolve into equilibrium points of LTS for the corresponding frequency.


\section{Alzheimer’s Disease}
\label{\detokenize{neuroscience/alzheimers-disease::doc}}\label{\detokenize{neuroscience/alzheimers-disease:alzheimer-s-disease}}

\subsection{Pathology}
\label{\detokenize{neuroscience/alzheimers-disease:pathology}}\begin{enumerate}
\item {} 
Amyloid-beta deposition in grey matter for Alzheimer’s disease. Accumulation of amyloid-beta then leads to oligomers which finally form plaques. It can \sphinxstylestrong{alter intracellular Ca2+ levels}. \phantomsection\label{\detokenize{neuroscience/alzheimers-disease:id1}}{\hyperref[\detokenize{neuroscience/alzheimers-disease:laferla2008}]{\sphinxcrossref{{[}LaFerla2008{]}}}}

\item {} 
Amyloid-beta can transmit between human \phantomsection\label{\detokenize{neuroscience/alzheimers-disease:id2}}{\hyperref[\detokenize{neuroscience/alzheimers-disease:jaunmuktane2015}]{\sphinxcrossref{{[}Jaunmuktane2015{]}}}}

\end{enumerate}


\subsection{Network Dysfunction}
\label{\detokenize{neuroscience/alzheimers-disease:network-dysfunction}}\begin{enumerate}
\item {} 
Hypersynchrony and altered oscillatory rhythmic activity \phantomsection\label{\detokenize{neuroscience/alzheimers-disease:id3}}{\hyperref[\detokenize{neuroscience/alzheimers-disease:palop2016}]{\sphinxcrossref{{[}Palop2016{]}}}}

\end{enumerate}


\section{Functional Connectivity}
\label{\detokenize{neuroscience/functional-connectivity::doc}}\label{\detokenize{neuroscience/functional-connectivity:functional-connectivity}}
Biological neural networks are quite complicated in the sense maps. Given some input the network generates a output, which is a map of the complicated kind. However, we could also feel the simplicity of such complicated networks. The building blocks of such network are simple neurons which are filters of information.

As a physicist, we have been working with harmonic oscillators. Single oscillators have its frequency of oscillations. Coupled oscillators have their eigenmode, which are hidden collective behaviors of the oscillators. It seems that each oscillator is oscillating chaotically. As we find the eigenmodes, we find the collective patterns. Each oscillator joins some if not all of the collective patterns.

This idea inspires us. If we look at the spiking of each neuron, we might not be able to identify the patterns. However, for coupled neurons, we have to find their collective modes to really SEE the hidden patterns. This idea works for very small networks, the eigenvalues and eigenvectors of which can be found easily. For super large networks, it becomes mission impossible.

In any case, we can persue the idea that the network is composed of basic functional units. There are many levels and aspects of brain connectivity \phantomsection\label{\detokenize{neuroscience/functional-connectivity:id1}}{\hyperref[\detokenize{neuroscience/functional-connectivity:brainconnectivity}]{\sphinxcrossref{{[}BrainConnectivity{]}}}}.
\begin{enumerate}
\item {} 
Graph theory:
- clustering techniques
- community detection, \sphinxstylestrong{which is quite interesting}.

\item {} 
Morphometric methods

\item {} 
Functional brain connectivity:
- computing cross-correlations in the time or frequency domain;
- mutual information;
- spectral coherence.

\end{enumerate}

\begin{sphinxadmonition}{note}{Community Detection}

The article \phantomsection\label{\detokenize{neuroscience/functional-connectivity:id2}}{\hyperref[\detokenize{neuroscience/functional-connectivity:brainconnectivity}]{\sphinxcrossref{{[}BrainConnectivity{]}}}} mentioned that \sphinxstylestrong{eigenspectrum} is used in community detection.

In social science and complex networks, community detection is a well researched field. A bunch of methods have been developped.
\begin{itemize}
\item {} 
Q modularity \phantomsection\label{\detokenize{neuroscience/functional-connectivity:id3}}{\hyperref[\detokenize{neuroscience/functional-connectivity:newman2016}]{\sphinxcrossref{{[}Newman2016{]}}}};

\item {} 
Hyperbolic mapping \phantomsection\label{\detokenize{neuroscience/functional-connectivity:id4}}{\hyperref[\detokenize{neuroscience/functional-connectivity:boguna2010}]{\sphinxcrossref{{[}Boguna2010{]}}}};

\item {} 
Statistical mechanics method such as Gibbs distribution \phantomsection\label{\detokenize{neuroscience/functional-connectivity:id5}}{\hyperref[\detokenize{neuroscience/functional-connectivity:zhang2014}]{\sphinxcrossref{{[}Zhang2014{]}}}};

\item {} 
…

\end{itemize}
\end{sphinxadmonition}

Here are some facts about brain network.
\begin{enumerate}
\item {} 
It’s similar to small world network \phantomsection\label{\detokenize{neuroscience/functional-connectivity:id6}}{\hyperref[\detokenize{neuroscience/functional-connectivity:brainconnectivity}]{\sphinxcrossref{{[}BrainConnectivity{]}}}}.

\item {} 
There are functional hubs \phantomsection\label{\detokenize{neuroscience/functional-connectivity:id7}}{\hyperref[\detokenize{neuroscience/functional-connectivity:brainconnectivity}]{\sphinxcrossref{{[}BrainConnectivity{]}}}}.

\item {} 
Segregation and integration might be key to a functional neural network \phantomsection\label{\detokenize{neuroscience/functional-connectivity:id8}}{\hyperref[\detokenize{neuroscience/functional-connectivity:brainconnectivity}]{\sphinxcrossref{{[}BrainConnectivity{]}}}}.

\end{enumerate}


\subsection{References}
\label{\detokenize{neuroscience/functional-connectivity:references}}

\section{Solitary Waves}
\label{\detokenize{neuroscience/solitary-waves::doc}}\label{\detokenize{neuroscience/solitary-waves:solitary-waves}}
Solitary waves can be an interesting way of propagating information. In neural networks, solitary waves were found decades ago.


\subsection{Solitary Waves}
\label{\detokenize{neuroscience/solitary-waves:id1}}
We briefly review solitary waves mathematically and physically.


\section{Signal Decomposition}
\label{\detokenize{neuroscience/signal-decomposition:signal-decomposition}}\label{\detokenize{neuroscience/signal-decomposition::doc}}
\begin{sphinxadmonition}{note}{Acknowledgement}

I had some of the ideas when Han Lu was explaining an experiment after one of my lunch. We share some of the ideas.
\end{sphinxadmonition}

Signals, electric potential or stimulations, are usually decomposed into different modes to study a system. The most prominant application is the measured signals, but stimulations should also be decomposed to study the response.

Why would we expect the decomposition of signal in a brain to have any significance? \sphinxstylestrong{For a neural system of tiny elements to really work out a complicated problem, I expect that some of the neurons have to work collectively in order to solve the problem.} Such collective phenomona might generate observable signals in fMRI etc if the collective neurons are clustering in space. In this case a spherical harmonic decomposition would be able to detect some of the features. Well, this is a little doubtful because the activities might not be global.


\subsection{Measurement}
\label{\detokenize{neuroscience/signal-decomposition:measurement}}
Application of spherical harmonics to EEG have resulted in interesting experiments \phantomsection\label{\detokenize{neuroscience/signal-decomposition:id1}}{\hyperref[\detokenize{neuroscience/signal-decomposition:sivakumar2016}]{\sphinxcrossref{{[}Sivakumar2016{]}}}}.

However, \phantomsection\label{\detokenize{neuroscience/signal-decomposition:id2}}{\hyperref[\detokenize{neuroscience/signal-decomposition:sivakumar2016}]{\sphinxcrossref{{[}Sivakumar2016{]}}}} only used it to decompose the surface of the brain, which is not the complete information. A full map of signal should be a field of 3 dimensional sphere. At each point, we can have information about
\begin{enumerate}
\item {} 
electric potential,

\item {} 
current with direction (flux of charge),

\item {} 
electric field with direction.

\end{enumerate}

For electric potential, we can set up a reference point and measure it, which means it’s a scalar and we have a number at each point. But for the other two with directions, they are vectors.

The decompositions can be done in different ways, according to the information we want to extract.
\begin{enumerate}
\item {} 
Globally decomposite the signal with spherical harmonics \(Y_{}\) at different radius, a multiple expansion for the whole brain; It requires \sphinxstylestrong{spin weighted spherical harmonics} etc for vector fields.

\item {} 
Locally expand the signal at each point using spherical harmonics; Only need the spherical harmonics.

\end{enumerate}


\subsection{Stimulation}
\label{\detokenize{neuroscience/signal-decomposition:stimulation}}
Stimulations could also be done in a similar way. We apply different modes and check if each mode plays a different role in the brain.

Theoretically, we might think the brain activity might be related to the modes, since each mode stands for different levels of activity in different regions. For example, if we have only \(l=0\) mode at different radius we might not be doing anything, since an isotropic brain is very likely to be a dead one. One of the regions have large activities means we have higher modes excited and since the brain has a fixed anatomy structure, a certain mode indicates activities in different regions.

It’s the same for stimulations. Generating a certain mode of electric field or potential will cause a stimulation in different regions.


\subsection{Thearetical Investigation}
\label{\detokenize{neuroscience/signal-decomposition:thearetical-investigation}}
We could find the conservation laws of a popularition in multiple expansion and identify the conserved quantities from population theory. This could be useful for simulations.


\section{Dispersion Relation}
\label{\detokenize{neuroscience/dispersion-relation:dispersion-relation}}\label{\detokenize{neuroscience/dispersion-relation::doc}}
A network of neurons forms a field with the field value being the potential and every other valuable measurable quantities. With this concept set up, we could define plane waves on this network and perform linear stability analysis.

Suppose we have a wave of the form \(g(c t - x)\) travelling, which can be Fourier transformed into many plane waves. If the velocity of the wave doesn’t depend on the frequency of each Fourier mode, the wave would remain whatever it started as, which is non-dispersive. However, many waves are dispersive and will disperse as they travels. The reason that the waves disperse is that the velocity of each Fourier modes \(A(\omega)e^{-i (\omega t - k x )}\) is different such that different Fourier modes will seperate out due to the difference in velocity.

Thus we could find out the velocity as a function of frequency to tell how dispersive the waves are, i.e., \(c(\omega)\) or \(c(k)\), where \(\omega\) and \(k\) are the frequency and wave number respectively.

In fact the phase velocity is defined as \(c(\omega) = \frac{\omega}{k}\) while group velocity is \(c_{g}(\omega) = \frac{d\omega}{dk}\). We now established the relations between \(\omega\) and \(k\), e.g., \(D(\omega,k)=0\). The relation \(D(\omega,k)=0\) is dubbed as \sphinxstylestrong{dispersion relation}.


\subsection{Refences and Notes}
\label{\detokenize{neuroscience/dispersion-relation:refences-and-notes}}

\section{MISC}
\label{\detokenize{neuroscience/misc::doc}}\label{\detokenize{neuroscience/misc:misc}}

\subsection{Competition betwen Subnetworks}
\label{\detokenize{neuroscience/misc:competition-betwen-subnetworks}}\begin{enumerate}
\item {} 
Winnerless Competion: network activity spontaneously switch between same type of subnetworks

\item {} 
Cell assemblies: some neurons in mammalian neocrotex connected specifically with higher connection probabilities and stronger weights.

\end{enumerate}


\subsection{RSN (Resting State Networks)}
\label{\detokenize{neuroscience/misc:rsn-resting-state-networks}}\label{\detokenize{neuroscience/misc:resting-state-networks}}
A series of functional networks maintain a ligh level of coherence even the brain is at rest \phantomsection\label{\detokenize{neuroscience/misc:id1}}{\hyperref[\detokenize{neuroscience/misc:deco2010}]{\sphinxcrossref{{[}Deco2010{]}}}}.


\subsection{Whole-Brain Computational Connectomics}
\label{\detokenize{neuroscience/misc:whole-brain-computational-connectomics}}
Whole-brain computational models help neuropsychiatric disorders {[}Deco2014{]}.


\chapter{Equation Solving in Neuroscience}
\label{\detokenize{equation-solving-in-neuroscience/index::doc}}\label{\detokenize{equation-solving-in-neuroscience/index:equation-solving-in-neuroscience}}
Step-by-step demonstration of equation solving in neuroscience, with examples of differential equations from real neuroscience problems.


\section{Green’s Function Method}
\label{\detokenize{equation-solving-in-neuroscience/green-function::doc}}\label{\detokenize{equation-solving-in-neuroscience/green-function:green-s-function-method}}

\subsection{Green’s Function Method}
\label{\detokenize{equation-solving-in-neuroscience/green-function:id1}}
In this section, we demonstrate Green’s function method of solving differential equations.


\subsubsection{Cable Equation}
\label{\detokenize{equation-solving-in-neuroscience/green-function:cable-equation}}
The cable equation is written as \phantomsection\label{\detokenize{equation-solving-in-neuroscience/green-function:id2}}{\hyperref[\detokenize{equation-solving-in-neuroscience/green-function:gerstner2002}]{\sphinxcrossref{{[}Gerstner2002{]}}}}
\phantomsection\label{\detokenize{equation-solving-in-neuroscience/green-function:equation-eqn-cable-equation-potential}}\begin{equation}\label{equation:equation-solving-in-neuroscience/green-function:eqn-cable-equation-potential}
\begin{split}\frac{\partial}{\partial t} u(t,x) = \frac{\partial^2}{\partial x^2} u(t,x) - u(t,x) + i_{e}(t,x),\end{split}
\end{equation}
or
\phantomsection\label{\detokenize{equation-solving-in-neuroscience/green-function:equation-eqn-cable-equation-current}}\begin{equation}\label{equation:equation-solving-in-neuroscience/green-function:eqn-cable-equation-current}
\begin{split}\frac{\partial}{\partial t} i(t,x) = \frac{\partial^2}{\partial x^2} i(t,x) - i(t,x) + \frac{\partial}{\partial x} i_e (t,x),\end{split}
\end{equation}
where \(t\), \(x\), \(i\), \(i_e\) are all renormalized unit less quantities. For the meaning and definition of them, ref. page 55 of Gerster 2002 \phantomsection\label{\detokenize{equation-solving-in-neuroscience/green-function:id3}}{\hyperref[\detokenize{equation-solving-in-neuroscience/green-function:gerstner2002}]{\sphinxcrossref{{[}Gerstner2002{]}}}}.


\subsubsection{Solutions to Cable Equation}
\label{\detokenize{equation-solving-in-neuroscience/green-function:solutions-to-cable-equation}}

\paragraph{Stationary Solution}
\label{\detokenize{equation-solving-in-neuroscience/green-function:stationary-solution}}
Equation \eqref{equation:equation-solving-in-neuroscience/green-function:eqn-cable-equation-potential} can be solved for stationary case, which is
\phantomsection\label{\detokenize{equation-solving-in-neuroscience/green-function:equation-eqn-cable-equation-stationary-equation-potential}}\begin{equation}\label{equation:equation-solving-in-neuroscience/green-function:eqn-cable-equation-stationary-equation-potential}
\begin{split}\frac{\partial^2}{\partial x^2} u(t,x) - u(t,x) =- i_{e}(t,x) .\end{split}
\end{equation}
While many methods can be used to solve second order nonhonogenerous differential equations, Green’s function is the most general and useful one.

\begin{sphinxadmonition}{note}{Definition of Green’s Function}

The idea of Green’s function is very simple. TO solve a general solution of equation
\phantomsection\label{\detokenize{equation-solving-in-neuroscience/green-function:equation-eqn-green-function-example}}\begin{equation}\label{equation:equation-solving-in-neuroscience/green-function:eqn-green-function-example}
\begin{split}\frac{d^2}{d x^2} y(x) + y(x) = f(x),\end{split}
\end{equation}
where \(f(x)\) is the source and some given boundary conditions. To save ink we define
\begin{equation*}
\begin{split}\hat L_x = \frac{d^2}{dx^2} + 1,\end{split}
\end{equation*}
which takes a function \(y(x)\) to \(f(x)\), i.e.,
\phantomsection\label{\detokenize{equation-solving-in-neuroscience/green-function:equation-eqn-green-function-example-1}}\begin{equation}\label{equation:equation-solving-in-neuroscience/green-function:eqn-green-function-example-1}
\begin{split}\hat L_x y(x) = f(x).\end{split}
\end{equation}
Now we define the Green’s function to be the solution of equation \eqref{equation:equation-solving-in-neuroscience/green-function:eqn-green-function-example-1} but replacing the source with delta function \(\delta(x-z)\)
\begin{equation*}
\begin{split}\hat L_x G(x,z) = \delta(z-x).\end{split}
\end{equation*}
Why do we define this function? The solution to equation \eqref{equation:equation-solving-in-neuroscience/green-function:eqn-green-function-example} is given by
\begin{equation*}
\begin{split}y(x) = \int G(x,z) f(z) dz.\end{split}
\end{equation*}
To verify this conclusion we plug it into the LHS of equation \eqref{equation:equation-solving-in-neuroscience/green-function:eqn-green-function-example}
\begin{equation*}
\begin{split}& \left(\frac{d^2}{dx^2} +1 \right) \int G(x,z) f(z) dz \\
=& \int \left[ \left(\frac{d^2}{dx^2} +1 \right) G(x,z) \right] f(z) dz \\
=& \int \delta(z-x) f(z) dz \\
=& f(x),\end{split}
\end{equation*}
in which we used one of the properties of Dirac delta distribution
\begin{equation*}
\begin{split}\int f(z) \delta(z-x) dz = f(x).\end{split}
\end{equation*}
Also note that delta function is even, i.e., \(\delta(-x) = \delta(x)\).

So all we need to do to find the solution to a standard second differential equation
\begin{equation*}
\begin{split}\left( \frac{d^2}{dx^2} + p(x) \frac{d}{dx} + q(x) \right)y(x) = f(x)\end{split}
\end{equation*}
is do the following.
\begin{enumerate}
\item {} 
Find the general form of Green’s function (GF) for operator for operator \(\hat L_x\).

\item {} 
Apply boundary condition (BC) to GF. This might be the most tricky part of this method. Any ways, for a BC of the form \(y(a)=0=y(b)\), we can just choose it to vanish at \(a\) and \(b\). Otherwise we can move this step to the end when no intuition is coming to our mind.

\item {} 
Continuity at \(n-2\) order of derivatives at point \(x=z\), that is

\end{enumerate}
\begin{equation*}
\begin{split}G^{(n-2)}(x,z) \vert_{x<z} = G^{(n-2)} \vert _{x>z} ,\qquad \text{at } x= z.\end{split}
\end{equation*}\begin{enumerate}
\setcounter{enumi}{3}
\item {} 
Discontinuity of the first order derivative at \(x=z\), i.e.,
\begin{equation*}
\begin{split}G^{(n-1)}(x,z)\vert_{x>z} - G^{(n-1)}(x,z)\vert_{x<z} = 1, \qquad \text{at } x= z.\end{split}
\end{equation*}
This condition comes from the fact that the integral of Dirac delta distribution is Heaviside step function.

\item {} 
Solve the coefficients to get the GF.

\item {} 
The solution to an inhomogeneous ODE  \(y(x)=f(x)\) is given immediately by
\begin{equation*}
\begin{split}y(x) = \int G(x,z) f(z) dz.\end{split}
\end{equation*}
If we haven’t done step 2 we know would have some unkown coefficients which can be determined by the BC.

\end{enumerate}
\end{sphinxadmonition}

\begin{sphinxadmonition}{note}{How to Find Green’s Function}

So we are bound to find Green’s function. Solving a nonhonogeneous equation with delta as source is as easy as solving homogeneous equations.

We do this by demonstrating an example differential equation. The problem we are going to solve is
\begin{equation*}
\begin{split}\left(\frac{d^2}{dx^2} + \frac{1}{4}\right) y(x) = f(x),\end{split}
\end{equation*}
with boundary condition
\phantomsection\label{\detokenize{equation-solving-in-neuroscience/green-function:equation-eqn-green-function-example2-bc}}\begin{equation}\label{equation:equation-solving-in-neuroscience/green-function:eqn-green-function-example2-bc}
\begin{split}y(0) = y(\pi) = 0.\end{split}
\end{equation}
For simplicity we define
\begin{equation*}
\begin{split}\hat L_x = \frac{d^2}{dx^2} + \frac{1}{4}.\end{split}
\end{equation*}
\sphinxstylestrong{First of all we find the GF associated with}
\begin{equation*}
\begin{split}\hat L_x G(x,z) = \delta(z-x).\end{split}
\end{equation*}
We just follow the steps.
\begin{enumerate}
\item {} 
The general solution to
\begin{equation*}
\begin{split}\hat L_x G(x,z) = 0\end{split}
\end{equation*}
is given by
\begin{equation*}
\begin{split}G(x,z) = \begin{cases}
A_1\cos (x/2) + B_1 \sin(x/2), & \qquad x \leq z, \\
A_2\cos (x/2) + B_2 \sin(x/2), & \qquad x \geq z.
\end{cases}\end{split}
\end{equation*}
\item {} 
Continuity at \(x=z\) for the 0th order derivatives,
\begin{equation*}
\begin{split}G(z_-,z) = G(z_+,z),\end{split}
\end{equation*}
which is exactly
\phantomsection\label{\detokenize{equation-solving-in-neuroscience/green-function:equation-eqn-green-function-example2-continuity}}\begin{equation}\label{equation:equation-solving-in-neuroscience/green-function:eqn-green-function-example2-continuity}
\begin{split}A_1\cos(z/2) + B_1 \sin(z/2) = A_2 \cos(z/2) + B_2\sin(z/2).\end{split}
\end{equation}
\item {} 
Discontinuity condition at 1st order derivatives,
\begin{equation*}
\begin{split}\left.\frac{d}{dx} G(x,z)  \right\vert_{x=z_+} - \left.\frac{d}{dx} G(x,z)  \right\vert_{x=z_-} = 1,\end{split}
\end{equation*}
which is
\phantomsection\label{\detokenize{equation-solving-in-neuroscience/green-function:equation-eqn-green-function-example2-discontinuity}}\begin{equation}\label{equation:equation-solving-in-neuroscience/green-function:eqn-green-function-example2-discontinuity}
\begin{split}-\frac{A_2}{2}\sin\frac{z}{2} + \frac{B_2}{2} \cos\frac{z}{2} - \left( -\frac{A_1}{2}\sin\frac{z}{2} + \frac{B_1}{2}\cos\frac{z}{2} \right) = 1\end{split}
\end{equation}
Now we combine \eqref{equation:equation-solving-in-neuroscience/green-function:eqn-green-function-example2-continuity} and \eqref{equation:equation-solving-in-neuroscience/green-function:eqn-green-function-example2-discontinuity} to eliminate two degrees of freedom. For example, we can solve out \(A_1\) and \(B_1\) as a function of all other coefficients. Here we have
\begin{equation*}
\begin{split}B_1 &= \frac{ - 2/\sin(z/2) }{\tan(z/2) + \cot(z/2)} + B_2 , \\
A_1 &= A_2 + B_2(\tan(z/2)-1) + \frac{2}{\sin(z/2) + \cot(z/2)\cos(z/2)}.\end{split}
\end{equation*}
\item {} 
Write down the form solution using \(y(x) = \int G(x,z) f(z) dz\). Then we still have two unknown free coefficients \(A_2\) and \(B_2\), which in fact is to be determined by the BC equation \eqref{equation:equation-solving-in-neuroscience/green-function:eqn-green-function-example2-bc}.

\end{enumerate}
\end{sphinxadmonition}

The stationary equation \eqref{equation:equation-solving-in-neuroscience/green-function:eqn-cable-equation-stationary-equation-potential} can be written as
\begin{equation*}
\begin{split}\hat L_x u(x) = - i_e(t,x),\end{split}
\end{equation*}
where \(\hat L_x = \frac{d^2}{dx^2} -1\). The boundary condition is the vanishing wave at infinity \(u(\pm\infty)=0\). As we are talking about stationary equation, the source should be time-independent, thus we take only a one dimension Dirac distribution \(\delta(x)\) to solve for GF.

The general Green’s function is %
\begin{footnote}[1]\sphinxAtStartFootnote
In fact this is can be obtained by using the Fourier transform method.
%
\end{footnote}
\begin{equation*}
\begin{split}G(x,x') = \begin{cases}
C_1 e^{-x} + D_1 e^{x}, & \qquad x\leq z,\\
C_2 e^{-x} + D_2 e^{x}, & \qquad x\geq z.
\end{cases}\end{split}
\end{equation*}
\begin{sphinxadmonition}{note}{Solving Homogeneous Equation}

The corresponding homogeneous equation is
\begin{equation*}
\begin{split}\frac{d^2}{dx^2} u(x) - u(x) = 0.\end{split}
\end{equation*}
To find the general solution we assume it has the form
\begin{equation*}
\begin{split}u(x) = A e^{\omega x},\end{split}
\end{equation*}
which is then plugged back into the equation,
\begin{equation*}
\begin{split}(\omega^2 - 1) u(x) = 0.\end{split}
\end{equation*}
We require \(\omega^2-1=0\) to make the solution most general, which leads to
\begin{equation*}
\begin{split}\omega = \pm 1.\end{split}
\end{equation*}
Finally we write down the general solution to this homogeneous equation,
\begin{equation*}
\begin{split}u(x) = C e^{x} + D e^{-x}.\end{split}
\end{equation*}\end{sphinxadmonition}

In this simple case, BC can be applied to Green’s function first %
\begin{footnote}[2]\sphinxAtStartFootnote
Because the only possibility to make the integral \(u(\pm\infty)=\int G(\pm\infty,x') dx'=0\) satisfy \(u(\pm\infty)=0\) is to make sure GF vanish on the boundaries.
%
\end{footnote}, which means
\begin{equation*}
\begin{split}G(-\infty,x') &= 0, \\
G(\infty,x') &= 0.\end{split}
\end{equation*}
These conditions can significantly simplify the GF,
\begin{equation*}
\begin{split}G(x,x') = \begin{cases}
D_1 e^{x}, & \qquad x<x',\\
C_2 e^{-x}, & \qquad x>x'.
\end{cases}\end{split}
\end{equation*}
Then we use the continuity condition and discontinuity condition,
\begin{equation*}
\begin{split}G(x'_-,x') - G(x'_+,x') &= 0\\
\left.\frac{d}{dx}G(x,x')\right\vert_{x=x'_+} - \left.\frac{d}{dx}G(x,x')\right\vert_{x=x'_-} &= 1,\end{split}
\end{equation*}
which is basically
\begin{equation*}
\begin{split}D_1 e^{x'} - C_2 e^{-x'} &= 0,\\
- C_2 e^{-x'} - D_1 e^{x'} & =1.\end{split}
\end{equation*}
Solving out the coefficients, we get
\begin{equation*}
\begin{split}D_1 & = \frac{1}{2}e^{-x'},\\
C_2 &= \frac{1}{2}e^{x'}.\end{split}
\end{equation*}
Then we reached the complete and final GF,
\begin{equation*}
\begin{split}G(x,x') = \begin{cases}
\frac{1}{2}e^{x-x'}, & \qquad x<x'\\
\frac{1}{2}e^{x' - x}. & \qquad x>x'
\end{cases}\end{split}
\end{equation*}
Given any general source \(-i_e(t,x)\), we can write down the solution
\begin{equation*}
\begin{split}u(x) = \int G(x,x') (-i_e(t,x') ) dx'.\end{split}
\end{equation*}
As a verification, we integrate out for \(i_e(t,x) = 1\),
\begin{equation*}
\begin{split}u(x) = -\int_{-\infty}^{x}  \frac{1}{2}e^{x'-x} dx' - \int_{x}^{\infty} \frac{1}{2}e^{x - x'} dx' = 1,\end{split}
\end{equation*}
which is exactly the solution given by Mathematica and makes sense.


\paragraph{Physical Meaning}
\label{\detokenize{equation-solving-in-neuroscience/green-function:physical-meaning}}
So far we have been dealing with math. What is the actual meaning of GF? To dive into this question we need to review the equation for GF, in this case,
\begin{equation*}
\begin{split}\left(\frac{d^2}{dx^2} -1\right) u(x) = \delta(x'-x).\end{split}
\end{equation*}
On the RHS, source term is a delta function, which is just a stimulation to the system at point \(x'\). The textbook shows a graph %
\begin{footnote}[3]\sphinxAtStartFootnote
Wulfram Gerstner and Werner M. Kistler, Spiking Neuron Models, (2002), Fig. 2.17.
%
\end{footnote} for the case \(x'=0\), where we see the stimulation is given for point \(x'=0\) and the potential drops as we deviate from the stimulated point.

In a stimulation-response system, one of the most important properties is the resonance width, or reaction width, which means the deviation required for the amplitude to drop to \(1/e\) of the peak value. In this stationary solution, the distance is 1 in renormalized unit. To transform back to to SI unit, recall that the characteristic length is this problem is \(\lambda = \sqrt{\frac{r_T}{r_L}}\).

Just to build a picture, this length is around %
\begin{footnote}[4]\sphinxAtStartFootnote
Since opening of ion channesl can significantly change the transverse conductivity, this estimation can change significantly in different situations.
%
\end{footnote}
\begin{equation*}
\begin{split}\lambda = \sqrt{ \frac{r_T}{r_L}} = \sqrt{ \frac{30\mathrm{k\Omega\cdot cm^2}/(2\pi \rho)}{ 100 \mathrm{k\Omega\cdot cm}/(2\pi \rho) } } = \sqrt{ \frac{5\times 10^{11} \mathrm{\Omega \cdot \mu m} }{ 3\times 10^{5} \mathrm{\Omega \cdot \mu m^{-1}}  } } = 1.2\mathrm{mm}\end{split}
\end{equation*}

\paragraph{Non-stationary Solution}
\label{\detokenize{equation-solving-in-neuroscience/green-function:non-stationary-solution}}
To solve the most general non-homogeneous cable equation even for non-stationary case, we have to introduce a two-dimensional Dirac distribution \(\delta^2(t,x) = \delta(t)\delta(x)\).

Green’s function for the most general case should satisfy \phantomsection\label{\detokenize{equation-solving-in-neuroscience/green-function:id12}}{\hyperref[\detokenize{equation-solving-in-neuroscience/green-function:gerstner2002}]{\sphinxcrossref{{[}Gerstner2002{]}}}}
\begin{equation*}
\begin{split}\frac{\partial}{\partial t} G(t,t';x,x') - \frac{\partial^2}{\partial x^2}  G(t,t';x,x') +  G(t,t';x,x') = \delta(t'-t)\delta(x'-x).\end{split}
\end{equation*}
Again to save ink we define
\begin{equation*}
\begin{split}\hat L_{t,x} = \hat L_t - \hat L_x,\end{split}
\end{equation*}
where \(\hat L_t = \frac{\partial}{\partial t}\) and \(\hat L_x = \frac{\partial^2}{\partial x^2} - 1\).

The trick is to solve for time dependence first by Fourier transforming the equation to frequency space. To achieve that, we define
\phantomsection\label{\detokenize{equation-solving-in-neuroscience/green-function:equation-eqn-green-function-fourier-transform}}\begin{equation}\label{equation:equation-solving-in-neuroscience/green-function:eqn-green-function-fourier-transform}
\begin{split}G(t,t';x,x') = \frac{1}{\sqrt{2\pi}}\int_{-\infty}^\infty G(t,t';k,x')e^{ikx}dk.\end{split}
\end{equation}
On the other hand, Dirac delta is Fourier transformed to
\phantomsection\label{\detokenize{equation-solving-in-neuroscience/green-function:equation-eqn-dirac-delta-fourier-transform}}\begin{equation}\label{equation:equation-solving-in-neuroscience/green-function:eqn-dirac-delta-fourier-transform}
\begin{split}\delta(k) = \frac{1}{\sqrt{2\pi}} \int_{-\infty}^\infty \delta(\bar x) e^{- ik \bar x} d\bar x = \frac{1}{\sqrt{2\pi}} ,\end{split}
\end{equation}
which infact gives one of the representations of Dirac delta distribution
\begin{equation*}
\begin{split}\delta(\bar x) = \frac{1}{\sqrt{2\pi}}\int_{-\infty}^\infty \frac{1}{\sqrt{2\pi}} e^{ik \bar x} dk .\end{split}
\end{equation*}
Applying the transforms of \eqref{equation:equation-solving-in-neuroscience/green-function:eqn-green-function-fourier-transform} and \eqref{equation:equation-solving-in-neuroscience/green-function:eqn-dirac-delta-fourier-transform} to the equation we have
\begin{equation*}
\begin{split}\hat L_{t,x}\left( \frac{1}{\sqrt{2\pi}}\int_{-\infty}^\infty G(t,t';k,x')  e^{i kx} dk \right) = \delta(t'-t) \frac{1}{\sqrt{2\pi}} \int_{-\infty}^\infty \frac{1}{\sqrt{2\pi}}  e^{i k(x'-x)} dk,\end{split}
\end{equation*}
which becomes
\begin{equation*}
\begin{split}&\frac{1}{\sqrt{2\pi}}\int_{-\infty}^\infty \frac{\partial}{\partial t}  G(t,t';k,x') e^{ikx}dk - \frac{1}{\sqrt{2\pi}}\int_{-\infty}^\infty  G(t,t';k,x') \frac{\partial^2}{\partial x^2} e^{ikx}dk  \\
&+  \frac{1}{\sqrt{2\pi}}\int_{-\infty}^\infty G(t,t';k,x')e^{ikx}dk = \delta(t'-t) \frac{1}{\sqrt{2\pi}} \int_{-\infty}^\infty \frac{1}{\sqrt{2\pi}} e^{ik(x'-x)} dk,\end{split}
\end{equation*}
which is then simplified by removing the integral and common parts
\begin{equation*}
\begin{split}\frac{\partial}{\partial t} G(t,t';k,x')  + k^2 G(t,t';k,x') + G(t,t';k,x') = \delta(t'-t) \frac{1}{\sqrt{2\pi}} .\end{split}
\end{equation*}
Then we solve this first order differential equation.


\section{Fourier Tranform}
\label{\detokenize{equation-solving-in-neuroscience/fourier-transform:fourier-tranform}}\label{\detokenize{equation-solving-in-neuroscience/fourier-transform::doc}}

\subsection{Fourier Serious and Continuous Fourier Transform}
\label{\detokenize{equation-solving-in-neuroscience/fourier-transform:fourier-serious-and-continuous-fourier-transform}}
A function \(f(x)\) defined on \(x\in [-L, L]\) can be decomposed into Fourier series
\begin{equation*}
\begin{split}f(x) = \sum_{-\infty}^\infty A_n e^{i n k_0 x },\end{split}
\end{equation*}
where
\begin{equation*}
\begin{split}k_0 = 2 \pi/ 2 L = \pi/L.\end{split}
\end{equation*}
A continuous Fourier transform is not very different from Fourier series for equation solving. A function \(f(x)\) can be written as a convolution of another function \(g(x)\),
\phantomsection\label{\detokenize{equation-solving-in-neuroscience/fourier-transform:equation-eqn-fourier-transform-original-1}}\begin{equation}\label{equation:equation-solving-in-neuroscience/fourier-transform:eqn-fourier-transform-original-1}
\begin{split}f(x) = \int_{-\infty}^\infty g(t) e^{2\pi i x t} dt,\end{split}
\end{equation}
where \(f(x)\) is the Fourier transform of \(g(t)\). It is called a \sphinxstylestrong{transform} because we have a change of variable from t to x. This form is not convinient for the purpose of equation solving. We usually write it as
\begin{equation*}
\begin{split}f(x) = \frac{1}{2\pi}\int_{-\infty}^\infty g(k) e^{i k x} dk,\end{split}
\end{equation*}
which is derived by plug \(k=2\pi t\) in to Eq. (\eqref{equation:equation-solving-in-neuroscience/fourier-transform:eqn-fourier-transform-original-1}).

\begin{sphinxadmonition}{note}{Some Tips about Differential Equations}

It’s useful to try out Fourier series and Fourier transform on some frequently used operators.

For Fourier series
\begin{equation*}
\begin{split}\frac{d}{dx}f(x) = \frac{d}{dx}\sum_{-\infty}^\infty A_n e^{i n k_0 x } = \sum_{-\infty}^\infty A_n \frac{d}{dx} e^{i n k_0 x } = \sum_{-\infty}^\infty (i n k_0) A_n  e^{i n k_0 x }.\end{split}
\end{equation*}
As we apply Fourier transform,
\begin{equation*}
\begin{split}\frac{d}{dx}f(x) = \frac{1}{2\pi}\int_{-\infty}^\infty g(t) \frac{d}{dx} e^{i x t} dt = \frac{1}{2\pi} \int_{-\infty}^\infty g(t) (i x t)e^{i x t} dt,\end{split}
\end{equation*}
which is more or less similar to the result from Fourier series.
\end{sphinxadmonition}


\subsection{Application in Equation Solving}
\label{\detokenize{equation-solving-in-neuroscience/fourier-transform:application-in-equation-solving}}
\sphinxstylestrong{We provide two examples. The first one is a homogenous equation that is NOT solved using this method. The second equation is a inhomogenous one.}

In the first example, we apply Fourier series and Fourier transform to the differential equation of harmonic oscillators,
\phantomsection\label{\detokenize{equation-solving-in-neuroscience/fourier-transform:equation-eqn-harmonic-oscillator-eom}}\begin{equation}\label{equation:equation-solving-in-neuroscience/fourier-transform:eqn-harmonic-oscillator-eom}
\begin{split}\ddot x(t) = - \omega^2 x(t),\end{split}
\end{equation}
where \(\dot {} = \frac{d}{dt}\).

The second example will be a damped oscillator, which is
\phantomsection\label{\detokenize{equation-solving-in-neuroscience/fourier-transform:equation-eqn-damped-harmonic-oscillator-eom}}\begin{equation}\label{equation:equation-solving-in-neuroscience/fourier-transform:eqn-damped-harmonic-oscillator-eom}
\begin{split}\ddot x(t) = - \omega^2 x(t) + f(t).\end{split}
\end{equation}

\subsubsection{Homogeneous Equations}
\label{\detokenize{equation-solving-in-neuroscience/fourier-transform:homogeneous-equations}}
We’ll find out that homogenous differential equations doesn’t have solutions of the Fourier series type. The reason is that the transform of the solution to a homogeneous equation diverges. However, we still love this example because it shows us that using this method doesn’t garantee that we will find a solution.


\paragraph{Using Fourier Series}
\label{\detokenize{equation-solving-in-neuroscience/fourier-transform:using-fourier-series}}
Insert Fourier series of \(x(t) = \sum_{-\infty}^{\infty} A_n e^{i n k_0 t }\) into equation of harmonic oscillator (\eqref{equation:equation-solving-in-neuroscience/fourier-transform:eqn-harmonic-oscillator-eom}),
\begin{equation*}
\begin{split}\ddot \sum_{-\infty}^{\infty} A_n e^{i n k_0 t } = - \omega^2 \sum_{-\infty}^{\infty} A_n e^{i n k_0 t }.\end{split}
\end{equation*}
The derivatives will only apply to term \(e^{i n k_0 t }\), so that the equations becomes,
\begin{equation*}
\begin{split}\sum_{-\infty}^{\infty} (i n k_0)^2 A_n e^{i n k_0 t } = - \omega^2 \sum_{-\infty}^{\infty} A_n e^{i n k_0 t }.\end{split}
\end{equation*}
We move everything to the left side and combine the summations,
\begin{equation*}
\begin{split}\sum_{-\infty}^{\infty} \left( (i n k_0)^2   +  \omega^2   \right)A_n e^{i n k_0 t } = 0.\end{split}
\end{equation*}
The only possible solution to this equation is that the coefficient is zero, i.e.,
\begin{equation*}
\begin{split}(i n k_0)^2   +  \omega^2  =0,\end{split}
\end{equation*}
which has a solution of interest
\begin{equation*}
\begin{split}k_0 =\pm \omega /n.\end{split}
\end{equation*}
This result is the dispersion relation like solution to the system. However, we can not construct the original solution from this relation only. The reason is that the equation is homogeneous.


\paragraph{Using Fourier Transform}
\label{\detokenize{equation-solving-in-neuroscience/fourier-transform:using-fourier-transform}}
We assume that the function \(x(t)\) that we are looking for is a Fourier transform of another function \(\hat x(\tau)\)
\begin{equation*}
\begin{split}x(t) = \frac{1}{2\pi} \int_{-\infty}^\infty \hat x(k) e^{i k t} dk.\end{split}
\end{equation*}
Insert this transform into the equation of harmonic oscillator (\eqref{equation:equation-solving-in-neuroscience/fourier-transform:eqn-harmonic-oscillator-eom}),
\begin{equation*}
\begin{split}\frac{d^2}{dt^2} \frac{1}{2\pi} \int_{-\infty}^\infty \hat x(k) e^{i k t} dk = - \omega^2 \frac{1}{2\pi} \int_{-\infty}^\infty \hat x(k) e^{i k t} dk,\end{split}
\end{equation*}
which is simplified to
\begin{equation*}
\begin{split}\frac{1}{2\pi} \int_{-\infty}^\infty \left(  (i k  )^2  + \omega^2 \right) \hat x(\tau) e^{ik t} dk .\end{split}
\end{equation*}
We find ourselves in the same situation as the Fourier series solution. No solution is found.


\subsubsection{Inhomogeneous Equations}
\label{\detokenize{equation-solving-in-neuroscience/fourier-transform:inhomogeneous-equations}}
For an inhomogeneous equation (\eqref{equation:equation-solving-in-neuroscience/fourier-transform:eqn-damped-harmonic-oscillator-eom}), we can apply the same trick. However, we use Fourier transform for generality. Please note that the Fourier transform requires the solution to be defined on a range of the argument.

Before we actually work out the case, here are some tricks. We always use hat to denote the inverse Fourier transformed equation. Fourier transform of the equation actually will be a replacement of the following,
\begin{equation*}
\begin{split}f(t)\to \hat f(k) \\
x(t)\to \hat x(k) \\
\dot x(t) \to (ik )\hat x(k) \\
\dot x(t) \to (ik )^2\hat x(k).\end{split}
\end{equation*}
Using these rules, we find out the Fourier transform equation of (\eqref{equation:equation-solving-in-neuroscience/fourier-transform:eqn-damped-harmonic-oscillator-eom}),
\begin{equation*}
\begin{split}(ik )^2 \hat x(k) + \omega^2  \hat x(k) = \hat f(k),\end{split}
\end{equation*}
which has a solution
\begin{equation*}
\begin{split}\hat x(k) = \frac{ \hat f(k) }{ -k^2 + \omega^2 },\end{split}
\end{equation*}
so that the final solution we are looking for becomes
\begin{equation*}
\begin{split}x(t) =& \frac{1}{2\pi}\int_{-\infty}^\infty \hat x(k) e^{i k t} dk \\
=& \frac{1}{2\pi}\int_{-\infty}^\infty  \frac{ \hat f(k) }{ -k^2 + \omega^2 } e^{i k t} dk.\end{split}
\end{equation*}
We know \(f(t)\) so \(\hat f(k)\) can be calculated.


\subsection{Application to Neuroscience}
\label{\detokenize{equation-solving-in-neuroscience/fourier-transform:application-to-neuroscience}}

\section{Laplace Tranform}
\label{\detokenize{equation-solving-in-neuroscience/laplace-transform::doc}}\label{\detokenize{equation-solving-in-neuroscience/laplace-transform:laplace-tranform}}

\subsection{Laplace Transform}
\label{\detokenize{equation-solving-in-neuroscience/laplace-transform:laplace-transform}}
Laplace transform is useful in equation solving. By definition, Laplace transform transforms a function \(f(x)\) defined on \(x\geq 0\) into another function by calculating the convolution
\begin{equation*}
\begin{split}F(s) = \int_0^\infty e^{-s x} f(x) dx,\end{split}
\end{equation*}
or symbolically,
\begin{equation*}
\begin{split}F(s) = \mathcal L_s (f(x)).\end{split}
\end{equation*}
A table of important Laplace transforms can be found on \sphinxhref{http://mathworld.wolfram.com/LaplaceTransform.html}{mathworld.wolfram.com}. Here we steal some of the commonly used.
\begin{equation*}
\begin{split}\mathcal L_s (1) =& \frac{1}{s}\\
\mathcal L_s (x^n) =& \frac{n!}{s^{n+1}}\\
\mathcal \sin(kx) =& \frac{k}{k^2+s^2} \\
\mathcal \cos(kx) =& \frac{s}{k^2+s^2}.\end{split}
\end{equation*}
It can also be applied to differentials.
\begin{equation*}
\begin{split}\mathcal L_s \left(\frac{d f(x)}{dx}\right) =& s \mathcal L_s (f(x)) - f(0) \\
\mathcal L_s \left(\frac{d^2f(x)}{dx^2}\right) =& s^2 \mathcal L_s (f(x)) - s f(0) - \left.\frac{df(x)}{dx}\right\vert_{x=0}.\end{split}
\end{equation*}
\begin{sphinxadmonition}{note}{Laplace Transform of Differentials}

The general form is
\begin{equation*}
\begin{split}\mathcal L_s (f^{(n)}(x)) = s^n \mathcal L (f(x)) - \sum_{m=1}^n s^{n-m} f^{(m-1)}(0).\end{split}
\end{equation*}\end{sphinxadmonition}

Integrals with upper limit as argument also transform nicely.
\begin{equation*}
\begin{split}\mathcal L_s \left( \int_0^x f(x') dx'  \right) = \frac{\mathcal L_s(f)}{s}.\end{split}
\end{equation*}
Laplace transform is useful in solving differential equations because it transforms many equations into fractions and polynomials. A simple example is the harmonic osicllators. The equation of motion is
\begin{equation*}
\begin{split}\ddot x(t) = - \omega^2 x(t),\end{split}
\end{equation*}
which is transformed into
\begin{equation*}
\begin{split}s^2 X(s) - s x(0) - \dot x(0) = - \omega^2 X(s).\end{split}
\end{equation*}
The solution to it is
\begin{equation*}
\begin{split}X(s) = \frac{ s x(0) + \dot x(0)}{s^2 + \omega^2} = x(0)\frac{ s }{s^2 + \omega^2} + \frac{\dot x(0)}{\omega} \frac{\omega}{s^2 + \omega^2}.\end{split}
\end{equation*}
We can spot sin and cos from the solution, or more generally perform an inverse Laplace transform,
\begin{equation*}
\begin{split}x(t) = x(0) \cos(\omega t) + \frac{\dot x(0)}{\omega} \sin(\omega t).\end{split}
\end{equation*}
This example is too simple sometimes naive. However, it shows the spirit.

\begin{sphinxadmonition}{note}{Caveats}

Laplace transform has a lot of counter intuitive expressions.
\begin{enumerate}
\item {} 
Laplace transform of product of two functions \(f(x)g(x)\) is \sphinxstylestrong{NOT} the product of the Laplace transforms \(\mathcal L_s(f)\mathcal L_s(g)\). However, if one of the functions is a constant, say \(f(x)=5\), we can prove that the Laplace transform of \(5g(x)\) is \(5\mathcal L_s(g)\).

\item {} 
The product of two Laplace transforms \(\mathcal L_s(f)\mathcal L_s(g)\) is the Laplace transform of a convolution
\begin{equation*}
\begin{split}\int_0^x f(x') g(x-x') dx'.\end{split}
\end{equation*}
\item {} 
Small s corresponds to large x, due to the nature of the exponential suppression in Laplace transform. For example, for small argument x, the function \(e^{k t}\) becomes almost 1, meanwhile, the Laplace transform of the function \(1/(s - k)\) becomes \(1/s\) under large s. We can see that the two limits are consistant since the Laplace transform of 1 is \(1/s\).

\item {} 
In so many circumstance the Laplace transform doesn’t exist simple because the integral doesn’t converge. Please beware of this and use the transform only when it exists.

\end{enumerate}
\end{sphinxadmonition}

\begin{sphinxadmonition}{note}{Inverse Laplace Transform}

We do not usually use the general form of inverse Laplace transform since we can find it in the table. Nevertheless we write it down here.
\begin{equation*}
\begin{split}f(x) = \frac{1}{2\pi i} \int_{-i\infty}^{i \infty} F(s) e^{sx}ds.\end{split}
\end{equation*}
By defining \(s=i s^\dagger\), we can rewrite the formula
\begin{equation*}
\begin{split}f(x) = \frac{1}{2\pi} \int_{-\infty}^{\infty} F(is^\dagger) e^{is^\dagger x} ds^\dagger .\end{split}
\end{equation*}\end{sphinxadmonition}

The more interesting application is to solve matrix differential equations. For any equations
\begin{equation*}
\begin{split}\partial_x \mathbf f(x) = \mathbf A \mathbf f(x),\end{split}
\end{equation*}
Laplace transform takes it to the form
\begin{equation*}
\begin{split}s \mathbf F(x) - f(0) = \mathbf A \mathbf F(s).\end{split}
\end{equation*}
The solution is
\begin{equation*}
\begin{split}\mathbf F(x) = \frac{1}{s \mathbf I - \mathbf A} \mathbf f(0) .\end{split}
\end{equation*}
So the final solution for \(f(x)\) is
\begin{equation*}
\begin{split}\mathbf f(x) = \mathcal L^{-1} \left(\frac{1}{s \mathbf I - \mathbf A} \right)\mathbf f(0).\end{split}
\end{equation*}
We could work out the Taylor expansion of solution,
\begin{equation*}
\begin{split}\frac{1}{s \mathbf I - \mathbf A} = \frac{\mathbf I}{s}+ \frac{\mathbf A}{s^2} + \frac{\mathbf A^2}{s^3} \cdots.\end{split}
\end{equation*}
The inverse Laplace transform can be done simply term by term,
\begin{equation*}
\begin{split}\mathcal L^{-1} \left(\frac{1}{s \mathbf I - \mathbf A} \right) = \mathbf I + x \mathbf A + \frac{1}{2!} (x \mathbf A)^2 + \cdots = e^{x \mathbf A}.\end{split}
\end{equation*}
Finally we obtain the formal solution of the system, which is
\begin{equation*}
\begin{split}\mathbf f(x) = \exp\left( x \mathbf A \right)\mathbf f(0).\end{split}
\end{equation*}
\begin{sphinxadmonition}{note}{Only Works for Constant Coefficients}

This result only works for constant coefficients. In general, if the matrix \(A\) depends on the argument \(x\), the solution can be systematically calculated using the so called \sphinxhref{https://en.wikipedia.org/wiki/Magnus\_expansion}{Magnus Expansion}. However, it is as tedious as a numerical solution.
\end{sphinxadmonition}


\subsection{Application to Neuroscience}
\label{\detokenize{equation-solving-in-neuroscience/laplace-transform:application-to-neuroscience}}

\chapter{Random Walks}
\label{\detokenize{random-walks/index::doc}}\label{\detokenize{random-walks/index:random-walks}}
I am learning about random walks.

\sphinxhref{https://www.santafe.edu/engage/learn/courses/random-walks}{Here is a set lectures by Sid Redner.}


\section{Random Walks}
\label{\detokenize{random-walks/random-walks::doc}}\label{\detokenize{random-walks/random-walks:random-walks}}
There exists several simple models of random walks. It can be either fixed stepsize but random directions or varying stepsize.


\subsection{Fixed Stepsize}
\label{\detokenize{random-walks/random-walks:fixed-stepsize}}
A simple model is that we assume each step is a step of size \(\epsilon\), but with random directions. The position of this random walker at step \(N\) is the summation of all the steps (vectors),
\begin{equation*}
\begin{split}\vec X = \sum_i^N x_i,\end{split}
\end{equation*}
where \(x_i\) is the vector that represents step i.

What we want to find out is the places that the random walker explored after \(N\) steps. The corresponding quantity that represents it is \(\sqrt{\langle \vec X^2 \rangle}\).

From the idea of random walk, we know that
\begin{equation*}
\begin{split}\langle \vec x_i \rangle =& 0 \\
\langle \vec x_i \cdot \vec x_j \rangle =& 0.\end{split}
\end{equation*}
Thus
\begin{equation*}
\begin{split}\langle \vec X^2 \rangle = & \sum_i \vec x_i\cdot \vec x_i \\
=& N \epsilon^2.\end{split}
\end{equation*}
Then we find out that
\phantomsection\label{\detokenize{random-walks/random-walks:equation-eqn-rms-distance}}\begin{equation}\label{equation:random-walks/random-walks:eqn-rms-distance}
\begin{split}\bar X = \sqrt{\langle \vec X^2 \rangle} = \sqrt{N}\epsilon.\end{split}
\end{equation}

\subsection{Significance of Dimension}
\label{\detokenize{random-walks/random-walks:significance-of-dimension}}
From the root-mean-squared distance Eq. \eqref{equation:random-walks/random-walks:eqn-rms-distance} we can define the density of points. Suppose we have a continues version of this random walk. After time \(t\), the random walker walked a distance \(vt\). Meanwhile the random walker explored a region of radius \(\sqrt{t}v\), which corresponds to a volume \(V \propto \sqrt{t}v\). The density of walked points is defined as
\begin{equation*}
\begin{split}\rho \propto \frac{t}{\sqrt{t}^d} = t^{1-d/2},\end{split}
\end{equation*}
where \(d\) is the dimension of the space.

We spot this critical dimension \(d=2\).
\begin{enumerate}
\item {} 
\(d<2\): the density of points at \(t\to\infty\) becomes \(\rho\to \infty\). This called recurrent behavior. We are sure that after infinite time, we are going back to a point that we visited before.

\item {} 
\(d=2\): the density of points at \(t\to\infty\) becomes \(\rho\to \mathrm{Constant}\). \sphinxstylestrong{This derivation is wrong about this critical case.} It should be \(\rho\to \ln t\)

\item {} 
\(d>2\): the density of points at \(t\to\infty\) becomes \(\rho\to 0\). This is called transit behavior. We are not sure that we could go back to a point that we visited before.

\end{enumerate}


\subsection{Refs \& Notes}
\label{\detokenize{random-walks/random-walks:refs-notes}}\begin{enumerate}
\item {} 
\sphinxhref{https://www.santafe.edu/engage/learn/courses/random-walks}{Sid Redner’s Lectures @ Santa Fe Institute}

\end{enumerate}


\chapter{Machine Intelligence}
\label{\detokenize{machine-intelligence/index::doc}}\label{\detokenize{machine-intelligence/index:machine-intelligence}}

\section{Artificial Neural Network}
\label{\detokenize{machine-intelligence/ann::doc}}\label{\detokenize{machine-intelligence/ann:artificial-neural-network}}

\subsection{Basics}
\label{\detokenize{machine-intelligence/ann:basics}}
Artificial neural network can be used as a universal approximator. We all had the experience of guess the solution to some particular problems. Artificial neural network serves as a general one that can be used to approximate a lot of functions.

The simplest example is that we can always describe a function \(f(x)\) as
\begin{equation*}
\begin{split}y(x) = \sum_{k=1}^{N} v_k * \text{activation}( w_k * x + u_k ),\end{split}
\end{equation*}
where \sphinxstylestrong{activation} is a function that is related to the property of single neurons. Precisely speaking, this decomposition of the function is a network composed of N neurons.

\begin{figure}[htbp]
\centering
\capstart

\noindent\sphinxincludegraphics{{neural-network-simple}.png}
\caption{A very simple artificial neural network.}\label{\detokenize{machine-intelligence/ann:id1}}\end{figure}

How exactly does it work?

\begin{figure}[htbp]
\centering
\capstart

\noindent\sphinxincludegraphics{{how-artificial-neural-network-works}.png}
\caption{Illustration of how ann works as an approximator.}\label{\detokenize{machine-intelligence/ann:id2}}\end{figure}

The only problem, however, is that we need to find out the value of the parameters, thus training. For training we either need some real data that should be approximated by this network or a conservation law.


\subsection{Solving Differential Equations}
\label{\detokenize{machine-intelligence/ann:solving-differential-equations}}
For a differential equation, we have a natural conservation law. For example, equation
\begin{equation*}
\begin{split}\frac{d}{dt}y(t)= - y(t)\end{split}
\end{equation*}
means that the quantity
\begin{equation*}
\begin{split}\frac{d}{dt}y(t) + y(t)\end{split}
\end{equation*}
is conserved and is exactly 0. If we ever try to use ANN to approximate the function \(y(t)\), this conservation law should be satisfied and it’s the only law that the approximator should obey.

Using the network, we know that for each argument \(t_i\), we should have an output
\begin{equation*}
\begin{split}y_i= 1+t_i v_k f(t_i w_k+u_k).\end{split}
\end{equation*}
By training, we are talking about minimization the deviation of the quantity from the conserved value. We devise a function that describes the deviation and name it the cost,
\begin{equation*}
\begin{split}I=\sum_i\left( \frac{dy_i}{dt}+y_i \right)^2.\end{split}
\end{equation*}
We could actually calculate \(dy/dt\) using the approximator,
\begin{equation*}
\begin{split}\frac{dy}{dt} = v_k f(t w_k+u_k) + t v_k f(tw_k+u_k) (1-f(tw_k+u_k))w_k,\end{split}
\end{equation*}
where we denote the \sphinxstylestrong{activation} function \(f\). Then we can parameterize the cost function using the parameters
\begin{equation*}
\begin{split}I = \sum_i \left(  v_k f(t w_k+u_k) + t v_k f(tw_k+u_k) (1-f(tw_k+u_k)) w_k + y \right)^2.\end{split}
\end{equation*}
The final step is to find the parameters so that this cost is minimized.

\begin{sphinxadmonition}{note}{Activation function or trigger function}

One of the useful activation function is
\begin{equation*}
\begin{split}f(x)=\frac{1}{1+\exp(-x)}.\end{split}
\end{equation*}\end{sphinxadmonition}


\section{Neural Networks and Finite Element Method}
\label{\detokenize{machine-intelligence/neural-network-and-fem::doc}}\label{\detokenize{machine-intelligence/neural-network-and-fem:neural-networks-and-finite-element-method}}
In some sense, feed forward neural network is similar to finite element method.


\section{Boltzmann Machine}
\label{\detokenize{machine-intelligence/boltzmann-machine::doc}}\label{\detokenize{machine-intelligence/boltzmann-machine:boltzmann-machine}}
Boltzmann machine is much like a spin glass model in physics. In short words, Boltzmann machine is a machine that has nodes that can take values, and the nodes are connected through some weight. It is just like any other neual nets but with complications and theoretical implications.


\subsection{Boltzmann Machine and Physics}
\label{\detokenize{machine-intelligence/boltzmann-machine:boltzmann-machine-and-physics}}
To obtain a good understanding of Boltzmann machine for a physicist, we begin with Ising model. We construct a system of neurons \(\{ s_i\}\) which can take values of 1 or -1, where each pair of them \(s_i\) and \(s_j\) is connected by weight \(J_{ij}\).

This is described as a Boltzmann machine, or spin glass in physics. Spin glass is a type of material that is a composite of many spins pointing in different directions. In principle spin glass is hard to calculate.

Neverthless we can make simplifications to this model. We require each spin to be connected to its nearest neighbours only. Such a model is called Ising model.

Intuitively, those spins can be viewed as tiny magnets that can point up or down only. Each spin interacts with its neighbours. These interactions are calculated in terms of energy,
\begin{equation*}
\begin{split}E = -\sum_{i,j} J_{ij} s_i s_j.\end{split}
\end{equation*}
Why do we care about energy? For a physics system, low energy means stable while high energy means unsatble since it might automatically change its configuration into low energy state. That being said, a system of spins is stable if the energy of all the interactions is low.

To find out a low energy state, one of the numerical methods is Monte Carlo method.

\begin{sphinxadmonition}{note}{States}

We have been talking about the word state without being specifying the definition of it. In fact we can think of two different pictures of states. For the purpose of this discussion, we consider a system of \(N\) particles and each of the particle has \(m\) degrees of freedom.

The first strategy is to set up a \(N\times m\) dimension space and describe the state of the whole system with on point in such a space. The distribution of the points can be determined by the corresponding categories of distribution functions. This is dubbed as \(\Gamma\) space.

The second strategy is to use a space of \(m\) dimensions where each particle of the system is a point in such a space. Such a space is called \(\mu\) space. In \(\mu\) space, the distribution of each particle state is calculated using BBGKY chain.

Once the macroscopic propertities of the system is assigned, the all possible states that leads to this macroscopic state show up with equal probability, aka, principle of \sphinxstylestrong{equal a priori probabilities}.
\end{sphinxadmonition}

\begin{sphinxadmonition}{note}{Partition Function}

Partition function \(Z\) is useful as we calculate the statistical properties of the network,
\begin{equation*}
\begin{split}Z = \sum e^{-E}.\end{split}
\end{equation*}
With partition function defined, the distribution of states is
\begin{equation*}
\begin{split}P = \frac{1}{Z} e^{-E}\end{split}
\end{equation*}\end{sphinxadmonition}


\subsection{Application of Boltzmann in Learning}
\label{\detokenize{machine-intelligence/boltzmann-machine:application-of-boltzmann-in-learning}}
In many cases, we would like the machine to learn about the pattern of input. Probabilistically speaking, we are working out the probability distribution of the input data using a Boltzmann machine,
\begin{equation*}
\begin{split}\Delta p =  p_{\mathrm{Model}} - p_{\mathrm{Data}}.\end{split}
\end{equation*}
Or for reasons of log-likelihood, we use the ration of them
\begin{equation*}
\begin{split}\Delta \log p =   \log p_{\mathrm{Model}} - \log p_{\mathrm{Data}}.\end{split}
\end{equation*}
In terms of Boltzmann machine weights, which are to be determined, the log probability is proportional to the energy
\begin{equation*}
\begin{split}\Delta \log p \sim \sum_{ij} w_{ij} s_i s_j \vert_{\mathrm{Model}}  -    \sum_{ij} w_{ij} s_i s_j \vert_{\mathrm{Data}}.\end{split}
\end{equation*}
Since we are looking for the weights, the updating rule should be equivalent to the gradient
\begin{equation*}
\begin{split}\frac{ \partial \Delta \log p }{ \partial w_{ij} } \sim  \langle s_i s_j\rangle_{\mathrm{data}} - \langle s_i s_j \rangle_{\mathrm{\model}},\end{split}
\end{equation*}
We could update our weights using a rule compatible with the gradient of the probability.
\phantomsection\label{\detokenize{machine-intelligence/boltzmann-machine:equation-eq-weight-update-rule-boltzmann-machine}}\begin{equation}\label{equation:machine-intelligence/boltzmann-machine:eq-weight-update-rule-boltzmann-machine}
\begin{split}\Delta w_{ij} \sim \langle s_i s_j\rangle_{\mathrm{data}} - \langle s_i s_j \rangle_{\mathrm{\model}}.\end{split}
\end{equation}
It’s easily noticed that the first term is basically Hebbian learning rule, where similar activities enhences the weight. The second term is the some unlearning rule where we have to reduce some weights to relax to the actual working network. Simply put, we kill some connects that have negative effects on our learning network.


\subsubsection{Without Hidden Units}
\label{\detokenize{machine-intelligence/boltzmann-machine:without-hidden-units}}
In principle, a learning process can be as simple as a one on one map from the data to all the neurons in Boltzmann machine.

Suppose we have a data set of an image which has 10 pixels and each pixel can take values of 0 and 1. We simply construct a network of 10 neurons. To remember the most prominent features of the image with this network, we update the weights and bias of the network to miminize the energy. Once done with minimization, the network we have could be used to generate similar images with similar features.


\subsubsection{With Hidden Units}
\label{\detokenize{machine-intelligence/boltzmann-machine:with-hidden-units}}
The complexity of the previous model seems to be low. To introduce more degree of freedom, we introduce the hidden units.

Hinton and Sejnowski worked out a algrimth for Boltzmann machine in 1983. The energy based learning rule for Boltzmann machine has two phases, the clamped phase (Phase+) and free phase (Phase-).
\begin{enumerate}
\item {} 
Clamped phase: we attach the data to the visible units and initialize the hidden units to be some random states.

\end{enumerate}
\begin{enumerate}
\item {} 
Then we update the hidden units so that the energy is minimized with clamping.

\item {} 
We calculate the average \(s_i s_j\) over all pairs of units, i.e., \(\langle s_i s_j\rangle_{\mathrm{data}}\).

\item {} 
Repeat for all data sets of training.

\end{enumerate}
\begin{enumerate}
\setcounter{enumi}{1}
\item {} 
Free phase: no clamping and all units are the same and mutatable.

\end{enumerate}
\begin{enumerate}
\item {} 
Relax the state of the units to low energy state.

\item {} 
Find out \(\langle s_i s_j\rangle_{\mathrm{model}}\).

\item {} 
Repeat N times.

\end{enumerate}

With \(\langle s_i s_j\rangle_{\mathrm{data}}\) and \(\langle s_i s_j\rangle_{\mathrm{model}}\) found, we use the update rule \eqref{equation:machine-intelligence/boltzmann-machine:eq-weight-update-rule-boltzmann-machine}.

For some reference, Hinton has a set of lectures on \sphinxhref{https://www.coursera.org/learn/neural-networks/lecture/iitiK/boltzmann-machine-learning-12-min}{Coursera}. He also has a lecture for more effective algrimths: \sphinxhref{https://www.coursera.org/learn/neural-networks/lecture/wlELo/optional-video-more-efficient-ways-to-get-the-statistics-15-mins}{More efficient ways to get the statistics}.


\subsection{Minimizing Energy of Ising Model is Hebbian Learning}
\label{\detokenize{machine-intelligence/boltzmann-machine:minimizing-energy-of-ising-model-is-hebbian-learning}}
\begin{sphinxadmonition}{note}{Hebbian Learning Rule}

Simply put, neurons act similarly at the same time would be more likely to be connected.
\end{sphinxadmonition}

A energy minimization procedure would be the same as Hebbian learning rule. Suppose we pick out two spins, \(s_3 = 1\) and \(s_8= 1\), the connected weight would be positive in order to have lower energy \(-J_{38}s_3 s_8 = - J_{38}\). For spins with different signs, negative weight would be the choice to make sure the energy is lower. This is similar to Hebbian learning rule.


\subsubsection{Programming}
\label{\detokenize{machine-intelligence/boltzmann-machine:programming}}
To code a Boltzmann machine, we need a protocal.


\chapter{Collective Intelligence}
\label{\detokenize{collective/index::doc}}\label{\detokenize{collective/index:collective-intelligence}}

\section{Collective intelligence References}
\label{\detokenize{collective/refs::doc}}\label{\detokenize{collective/refs:collective-intelligence-references}}\begin{enumerate}
\item {} \begin{description}
\item[{\sphinxhref{http://cci.mit.edu/}{MIT Center for Collective Intelligence}.}] \leavevmode
“Our basic research question is: How can people and computers be connected so that—collectively—they act more intelligently than any person, group, or computer has ever done before?”

\end{description}

\item {} 
\sphinxhref{https://en.wikipedia.org/wiki/Collective\_intelligence}{Collective intelligence@Wikipedia}

\end{enumerate}


\chapter{Some Topics}
\label{\detokenize{topics/index::doc}}\label{\detokenize{topics/index:some-topics}}
Some random but interesting topics.


\section{Mean-field Method}
\label{\detokenize{topics/mean-field::doc}}\label{\detokenize{topics/mean-field:mean-field-method}}
Mean-field is a concept in statistical mechanics to approximately treat the many body problem. It is an effective theory.


\subsection{Paramagnetic Material}
\label{\detokenize{topics/mean-field:paramagnetic-material}}
In paramagnetic materials, the “polarized” spins \(\{s_i\}\) determine the overall “magnetization” of the material. When the spins are all aligned, we get the maximium “magnetization”. If the directions of the spins are completely random, the material is not “magnetized”. In other words, we can use a quantity \(m=\langle s \rangle\), which is called magnetization, to represent how well the overall “magnetization” of the material is.

For simplicity, we assume that the spins can only be up (s=1) or down (s=-1).

\sphinxstylestrong{The idea of mean-field is to treat each spin as a mean-field} \(m\) \sphinxstylestrong{and the fluctuations around this mean-field}. Mathematically speaking
\begin{equation*}
\begin{split}s_i = m + \delta s_i,\end{split}
\end{equation*}
where \(\delta s_i = s_i -m\) is the so called fluctuations. This treatment of the material requires the material to be homogeneous. Under the condition of homogeneous material, we can think of all the spins are the same. Thus we can look at one spin \(s_\alpha\) and all other spins become a background environment of this particular spin. This background environment is called mean-field which provides a potential field to interact with.

On the other hand, the average of spins \(\langle s \rangle\) is equivalent to the ensemble average of this particular spin \(\langle s_\alpha \rangle\)
\begin{equation*}
\begin{split}\langle  s \rangle = \langle s_\alpha \rangle_{ensemble} = \sum P(E_\alpha) s_\alpha,\end{split}
\end{equation*}
where \(P(E_\alpha)\) is calculated by the Boltzmann distribution,
\begin{equation*}
\begin{split}P(E_\alpha) = \frac{e^{\beta E_\alpha} }{  \sum e^{\beta E_\alpha} }.\end{split}
\end{equation*}
So we need to calculate the energy of the spin first. Using the Hamiltonian for spins
\begin{equation*}
\begin{split}E_\alpha = - s_\alpha H_{total},\end{split}
\end{equation*}
where \(H_{total}\) is the total magnetic field including the mean-field. This energy finally becomes
\begin{equation*}
\begin{split}E_\alpha &= - s_\alpha \left( H_{ext} + J \sum_{i, near \alpha} s_i \right) \\
&= -s_\alpha \left( H_{ext} + J \sum_{i, near \alpha} ( m + (s_i - m) ) \right) \\
& = -s_\alpha \left( H_{ext} + J z m + J \sum_{i, near \alpha} ( s_i - m ) \right),\end{split}
\end{equation*}
where \(z\) is the number of spins interacting with \(s_\alpha\).

\sphinxstylestrong{Mean-field theory assume a homogeneous background, the summation of fluctuations}, \(\sum_{i, near \alpha} ( s_i - m)\) \sphinxstylestrong{should be 0}.

Then energy is calcuated as
\begin{equation*}
\begin{split}E_\alpha = - s_\alpha (z J m + H).\end{split}
\end{equation*}
Using this we get the ensemble average of spin \(\alpha\),
\begin{equation*}
\begin{split}\langle s_\alpha \rangle &= \sum_{s_\alpha}  s_\alpha \frac{e^{\beta E_\alpha} }{  \sum e^{\beta E_\alpha} } \\
&= \tanh \left( \beta (z J m + H) \right).\end{split}
\end{equation*}
By definition, we know that
\begin{equation*}
\begin{split}m = \langle s \rangle = \langle s_\alpha \rangle_{ensemble} = \tanh \left( \beta (z J m + H) \right).\end{split}
\end{equation*}
Such a transcendental equation tells us the magnetization of the material.


\subsection{Neuroscience}
\label{\detokenize{topics/mean-field:neuroscience}}
Mean-field theory is also applied to networks of neurons. Since it is an effective theory, we can only talk about the statistical features of neuron networks as mean-field is applied.

This method is explained in \phantomsection\label{\detokenize{topics/mean-field:id1}}{\hyperref[\detokenize{topics/mean-field:deco2008}]{\sphinxcrossref{{[}Deco2008{]}}}}. This section is an interpretation of it if not a rewritten.


\subsubsection{Two Phase Spaces}
\label{\detokenize{topics/mean-field:two-phase-spaces}}
Similar to the \(\mu\) space (6 D) and \(\Gamma\) space (6N D) in statistical mechanics, it takes a lot of dimensions if we would like to describe a neuron network using a point in a phase space (\(\Gamma\) space of network).

To see this, consider a network of \(N\) neurons, each neuron requires \(\zeta\) variables \(\{ \xi_1, \xi_2,\cdots, \xi_\zeta\}\) for a complete description (say, PSP \(V\), current \(I\), \(\mathcal T\), as mentioned in \phantomsection\label{\detokenize{topics/mean-field:id2}}{\hyperref[\detokenize{topics/mean-field:deco2008}]{\sphinxcrossref{{[}Deco2008{]}}}}). If we would like to describe the network using these \(\zeta\) variables, it is represented as a bunch of points in such a phase space, which we name \(\mu\) space. On the other hand, we can construct a \(N\zeta\) dimensional phase space, where each dimension is a variable of a certain neuron. Suppose the neurons are given ids \({}_i\). The dimensions are \(\xi_{j,i}\) which stands for the \(\xi_j\) variable of neuron i.

We could imagine how convoluted the new phase space (\(\Gamma\) space) has become.

In any case, the evolution of the network is mapped to the motion of a point in \(\Gamma\) space or motion of \(N\) points in \(\mu\) space.


\subsubsection{Equation of Motion}
\label{\detokenize{topics/mean-field:equation-of-motion}}
The dynamics of the system is described using Fokker-Planck equation,
\begin{equation*}
\begin{split}\partial_t p = - \nabla\cdot (f-D \nabla) p,\end{split}
\end{equation*}
where \(p\) is the density of points in phase space. This is a statistical formalism.

\begin{sphinxadmonition}{note}{Drift and Diffusion}

The two terms on the RHS are dubbed as drift term and diffusion term for the physics they are describing.
\end{sphinxadmonition}

\phantomsection\label{\detokenize{topics/mean-field:id3}}{\hyperref[\detokenize{topics/mean-field:deco2008}]{\sphinxcrossref{{[}Deco2008{]}}}} uses leaky integrate-and-fire model, which is used to find out the evolution of the membrane potential \(V_i\) of neuron \(i\), which has a solution
\begin{equation*}
\begin{split}V_i = V_L + \sum_{j=1}^N J_{ij} e^{-(t-t_j^{(k)})/\tau} \sum_k \Theta(t-t_j^{(k)}),\end{split}
\end{equation*}
where \(\Theta\) is the Heaviside function. The complication of this model comes from the fact that all neurons are potentially coupled.

\begin{sphinxadmonition}{note}{Leaky Integrate-and-fire Model}

LIF is a model utilizing capacitors and resistors to describe the evolution of membrane potential. The point of a capacitor is to reset the potential when it reaches the threshold.

The EoM is
\begin{equation*}
\begin{split}\tau \frac{d}{dt} V = - (V - V_L) + IR,\end{split}
\end{equation*}
where \(I\) is the total synaptic current input of the neuron, \(V_L\) is the leak.

For a neuron \(i\) in a network, \(I\) comes from all the synaptic delta pulse inputs of other neurons \(j\),
\begin{equation*}
\begin{split}I_i R = \tau \sum_{j=1}^N J_{ij} \sum_k \delta(t-t_j^{(k)}),\end{split}
\end{equation*}
where \(t_j^{(k)}\) denotes the time for kth spike of neuron \(j\).

The equation can be solved using Green’s function method.
\end{sphinxadmonition}

Following the idea of phase space, the authors of \phantomsection\label{\detokenize{topics/mean-field:id4}}{\hyperref[\detokenize{topics/mean-field:deco2008}]{\sphinxcrossref{{[}Deco2008{]}}}} derived Fokker-Planck equation for spiking neuron models. We could imagine that the \(\Gamma\) space for LIF models would be a \(N\) dimensional one \(\{V_i\}\), where \(N\) is the number of neurons. On the other hand, \(\mu\) space description requires only 1 D, which is potential. In \(\mu\) space, we define a probability density \(p(v,t)\), which gives us the probability of neuron falls in a potential range \([v,v+dv]\) when multiplied by the size of the phase space \(dv\).

The Chapman-Kolmogorov equation describes the evolution of such a density function
\begin{equation*}
\begin{split}p(v,t+dt) = \int_{-\infty}^\infty p(v-\epsilon,t)\rho(\epsilon\vert v-\epsilon) d\epsilon,\end{split}
\end{equation*}
where \(\rho(\epsilon\vert v-\epsilon)\) is the probability of neuron to be with potential \(v\) at \(t+dt\) given neuron potential at \(t\) to be \(v-\epsilon\). But this integral form is not always useful. Kramers-Moyal expansion of it gives us the differential form
\phantomsection\label{\detokenize{topics/mean-field:equation-eqn-prob-density-equation-with-kramers-moyal-expansion}}\begin{equation}\label{equation:topics/mean-field:eqn-prob-density-equation-with-kramers-moyal-expansion}
\begin{split}\partial_t p(v,t) = \sum_{k=1}^\infty \frac{ (-1)^k }{k!} \frac{\partial^k}{\partial v^k} \left( p(v,t) \lim_{dt\to 0} \frac{ \langle \epsilon^k\rangle_v }{dt} \right),\end{split}
\end{equation}
where
\begin{equation*}
\begin{split}\langle \epsilon^k\rangle_v = \int_{-\infty}^\infty \epsilon^k\rho(\epsilon\vert v) d\epsilon,\end{split}
\end{equation*}
which is an average.

The plan is to find method to calcualte \(\langle \epsilon^k \rangle_v\) which is an average with respect to \(v\).


\subsubsection{Mean-field Approximation}
\label{\detokenize{topics/mean-field:mean-field-approximation}}
By assuming each neurons are identical in this network, we face the same situation as in the paramagnetic material. By saying so we are going to assume that the time average (system average, in paramagnetic materials) is the same as the ensemble average, which treats all other neurons as a background and consider only one neuron.

The key of this practice is that \(\langle \epsilon^k \rangle_v = \langle \epsilon^k \rangle_{ensemble}\). On the other hand, we know that \(\langle \epsilon^k \rangle_{ensemble} = dV\), if we find this \(dV\) that calculates the change of potential.

We need to find a quantity that is useful to describe the network, which should be similar to our magnetization \(m\). The choice can either be average potential of each neuron \(V_{avg}\) or the total membrane potential \(V=NV_{avg}\). The authors in \phantomsection\label{\detokenize{topics/mean-field:id5}}{\hyperref[\detokenize{topics/mean-field:deco2008}]{\sphinxcrossref{{[}Deco2008{]}}}} choose the average membrane potential.

We denote mean firing rate as \(A\). The equation of \(V\) is
\begin{equation*}
\begin{split}dV = \langle J\rangle N A dt - \frac{V-V_L}{\tau} dt,\end{split}
\end{equation*}
where \(\langle J\rangle\) is the avarage synaptic weight and mean firing rate or population activity is
\begin{equation*}
\begin{split}A = \lim_{dt\to 0} \frac{ \text{number of spikes within time } [t,t+dt] }{dt} \frac{1}{N}.\end{split}
\end{equation*}
Since \(\epsilon=dV\), we plug it in to Eq. \eqref{equation:topics/mean-field:eqn-prob-density-equation-with-kramers-moyal-expansion}. To save keystrokes, we calcualte the moments first.
\begin{equation*}
\begin{split}M^{(k)} &= \lim_{dt\to 0} \frac{1}{dt}\langle \epsilon\rangle_v \\
& = \lim_{dt\to 0} \frac{1}{dt}\langle \epsilon\rangle_{ensemble} \\
& = \lim_{dt\to 0} \frac{1}{dt} dV \\
& = \cdots \\
& = \langle J^k \rangle N A.\end{split}
\end{equation*}
In the derivation we dropped all terms that contains order of infinitesimal varibales \(\mathcal O (dt^2)\).

The diffusion approximation will help us drop the \(k>2\) moments. This is a Taylor expansion anyway.

\begin{sphinxadmonition}{note}{A Simple Argument for Diffusion Approximation?}

There should be one.

Regardless of real understanding or not,
\begin{equation*}
\begin{split}M^{(1)} &= \frac{\mu(t)}{\tau} - \frac{ v-V_L }{\tau} \\
M^{(2)} &= \frac{\sigma^2(t)}{\tau},\end{split}
\end{equation*}
are related to the drift coefficient \(\mu(t)\) and the diffusion coefficient \(\sigma(t)\).
\end{sphinxadmonition}

Then we obtain the Fokker-Planck equation
\begin{equation*}
\begin{split}\partial_t  p(v,t) = \frac{1}{2\tau} \sigma^2(t) \partial^2_v p(v,t) + \partial_v \left[ \frac{ v - V_L - \mu(t) }{\tau} p(v,t) \right],\end{split}
\end{equation*}
where \(\sigma(t)\) is diffusion coefficient and \(\mu(t)\) is drift coefficient.


\subsection{References and Notes}
\label{\detokenize{topics/mean-field:references-and-notes}}

\section{Spin Glass}
\label{\detokenize{topics/spin-glass::doc}}\label{\detokenize{topics/spin-glass:spin-glass}}
In the realm of physics, Ising model has been and probably will be an useful model for our intuition. Naively speaking, our world is full of local interactions yet the consequences are very likely to be long range through long range correlations. The famous Ising model is a key model of such concepts.

Ising model deals with a system of spins and their interactions. To build up the dynamics of a system, we need the configuration of it as well as the interactions. In Ising mode, a bunch of spins are described by \(\{ s_i \}\), where \(s_i\) could be scalars that take discrete values or vectors phenomenologically. For simplicity we take the path of discretized values, \(s_i=1,-1\). The interactions are two body interactions is related to the descriptioin of energy
\begin{equation*}
\begin{split}H = - \sum_{i,j} J_{i,j} s_i s_j,\end{split}
\end{equation*}
where \(J_{i,j}\) indicates the strength of the interactions between the two spins.


\section{Binary Human Model}
\label{\detokenize{topics/binary-human-model::doc}}\label{\detokenize{topics/binary-human-model:binary-human-model}}
Han has been discussing with me about building human dynamics by making analogy to neural systems.

The idea is to treat every human as an activation function which is connected to each other to form a large network. The trouble was how to deal with information input and output from outside of the group which is similar to external stimulation in neural systems.


\subsection{A Simplified Model}
\label{\detokenize{topics/binary-human-model:a-simplified-model}}
Here we develop a model for wechat groups instead of general human dynamics. We assume everyone share all the information in the group.

There are several aspects to be defined.
\begin{enumerate}
\setcounter{enumi}{-1}
\item {} 
One way to simplify the model is to set up a external pool of information and opinions which we denote as \(\mathscr P\).

\item {} 
An opinion is plugged into the pool \(\mathscr P\) initially.

\item {} 
Everyone will take information and output information to this poll \(\mathscr P\).

\item {} 
Each human would either output null (being silent) or 1 (agreement) or 0 (disagreement). Each of the outputs in the pool is denoted as \(P_i\).

\item {} 
The output of each human depends on the pool, the nature of the human, and the relation with other people. We approximate each human using a binary input/output function.

\end{enumerate}

Such a general model seems to be good enough. However, we have to define all different types of human first.
\begin{enumerate}
\item {} 
Being average: takes in all the opinions of other participants and work out an average.
\begin{equation*}
\begin{split}\Theta(\frac{\sum_{1}^N P_i}{N}-0.5),\end{split}
\end{equation*}
where \(\Theta\) is the step function or Heaviside function.

\item {} 
Being extreme: exaggerates whatever is obtained from average。
\begin{equation*}
\begin{split}\Theta(\alpha\frac{\sum_{1}^N P_i}{N}-0.5),\end{split}
\end{equation*}
where \(\alpha\) is the extreme factor. For linear summations (euclidean metric), this is effectively pessimism (\(\alpha<1\)) or optimism (\(\alpha>1\)).

\item {} 
Synchronized to other people. The simplest case is to synchronize to one of the members in the group.

\end{enumerate}


\subsubsection{Numerical Model}
\label{\detokenize{topics/binary-human-model:numerical-model}}
We need to build a program to calculate an example and see if it makes sense.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{import} \PYG{n+nn}{numpy} \PYG{k}{as} \PYG{n+nn}{np}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{c+c1}{\PYGZsh{} Define Heaviside function; returns 0 if x\PYGZlt{}0.5, otherwise returns 1}

\PYG{k}{def} \PYG{n+nf}{hsh}\PYG{p}{(}\PYG{n}{x}\PYG{p}{)}\PYG{p}{:}
    \PYG{c+c1}{\PYGZsh{}return (0.5 * (np.sign(x\PYGZhy{}0.5) + 1))}
    \PYG{k}{return} \PYG{n+nb}{int}\PYG{p}{(}\PYG{n}{np}\PYG{o}{.}\PYG{n}{piecewise}\PYG{p}{(}\PYG{n}{x}\PYG{p}{,} \PYG{p}{[}\PYG{n}{x} \PYG{o}{\PYGZlt{}} \PYG{l+m+mf}{0.5}\PYG{p}{,} \PYG{n}{x} \PYG{o}{\PYGZgt{}}\PYG{o}{=} \PYG{l+m+mf}{0.5}\PYG{p}{]}\PYG{p}{,} \PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{)} \PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{c+c1}{\PYGZsh{} Test of functions}

\PYG{n+nb}{print} \PYG{n}{hsh}\PYG{p}{(}\PYG{l+m+mf}{0.1}\PYG{p}{)}\PYG{p}{,} \PYG{n}{hsh}\PYG{p}{(}\PYG{l+m+mf}{0.5}\PYG{p}{)}\PYG{p}{,} \PYG{n}{hsh}\PYG{p}{(}\PYG{l+m+mf}{0.6}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{l+m+mi}{0} \PYG{l+m+mi}{1} \PYG{l+m+mi}{1}
\end{sphinxVerbatim}

In this example, we simulate the opinion stream of each person as well
as the whole system which is determined by the pool.

For convinience we establish a group of 4 person.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{c+c1}{\PYGZsh{} Define the pool}

\PYG{n}{pool} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{random}\PYG{o}{.}\PYG{n}{randint}\PYG{p}{(}\PYG{l+m+mi}{2}\PYG{p}{,} \PYG{n}{size}\PYG{o}{=}\PYG{l+m+mi}{4}\PYG{p}{)}\PYG{o}{.}\PYG{n}{tolist}\PYG{p}{(}\PYG{p}{)}\PYG{p}{;}
\PYG{n+nb}{print} \PYG{n}{pool}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{1}\PYG{p}{]}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{c+c1}{\PYGZsh{} Define four characters of the group}

\PYG{c+c1}{\PYGZsh{}\PYGZsh{} First character is an averagist}

\PYG{k}{def} \PYG{n+nf}{avgist}\PYG{p}{(}\PYG{n}{x}\PYG{p}{)}\PYG{p}{:}

    \PYG{k}{return} \PYG{n}{hsh}\PYG{p}{(} \PYG{n}{np}\PYG{o}{.}\PYG{n}{sum}\PYG{p}{(}\PYG{n}{x}\PYG{p}{)}\PYG{o}{/}\PYG{n+nb}{len}\PYG{p}{(}\PYG{n}{x}\PYG{p}{)} \PYG{p}{)}

\PYG{c+c1}{\PYGZsh{}\PYGZsh{} The second character is pessimist}

\PYG{k}{def} \PYG{n+nf}{pesist}\PYG{p}{(}\PYG{n}{x}\PYG{p}{,}\PYG{n}{alpha}\PYG{o}{=}\PYG{l+m+mf}{0.6}\PYG{p}{)}\PYG{p}{:}

    \PYG{k}{return} \PYG{n}{hsh}\PYG{p}{(} \PYG{n}{alpha} \PYG{o}{*} \PYG{n}{np}\PYG{o}{.}\PYG{n}{sum}\PYG{p}{(}\PYG{n}{x}\PYG{p}{)}\PYG{o}{/}\PYG{n+nb}{len}\PYG{p}{(}\PYG{n}{x}\PYG{p}{)} \PYG{p}{)}


\PYG{c+c1}{\PYGZsh{}\PYGZsh{} The third character is a optimist}
\PYG{k}{def} \PYG{n+nf}{optist}\PYG{p}{(}\PYG{n}{x}\PYG{p}{,}\PYG{n}{alpha}\PYG{o}{=}\PYG{l+m+mi}{2}\PYG{p}{)}\PYG{p}{:}

    \PYG{k}{return} \PYG{n}{hsh}\PYG{p}{(} \PYG{n}{alpha} \PYG{o}{*} \PYG{n}{np}\PYG{o}{.}\PYG{n}{sum}\PYG{p}{(}\PYG{n}{x}\PYG{p}{)}\PYG{o}{/}\PYG{n+nb}{len}\PYG{p}{(}\PYG{n}{x}\PYG{p}{)} \PYG{p}{)}

\PYG{c+c1}{\PYGZsh{}\PYGZsh{} The fourth synchronizes with the first character, i.e., averagist}

\PYG{k}{def} \PYG{n+nf}{synist}\PYG{p}{(}\PYG{n}{x}\PYG{p}{)}\PYG{p}{:}

    \PYG{k}{return} \PYG{n}{x}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{c+c1}{\PYGZsh{} Test characters}

\PYG{n+nb}{print} \PYG{n}{avgist}\PYG{p}{(}\PYG{n}{pool}\PYG{p}{)}\PYG{p}{,} \PYG{n}{pesist}\PYG{p}{(}\PYG{n}{pool}\PYG{p}{)}\PYG{p}{,} \PYG{n}{optist}\PYG{p}{(}\PYG{n}{pool}\PYG{p}{)}\PYG{p}{,} \PYG{n}{synist}\PYG{p}{(}\PYG{n}{pool}\PYG{p}{)}
\PYG{n+nb}{print} \PYG{n}{np}\PYG{o}{.}\PYG{n}{array}\PYG{p}{(}\PYG{p}{[}\PYG{n}{avgist}\PYG{p}{(}\PYG{n}{pool}\PYG{p}{)}\PYG{p}{,} \PYG{n}{pesist}\PYG{p}{(}\PYG{n}{pool}\PYG{p}{)}\PYG{p}{,} \PYG{n}{optist}\PYG{p}{(}\PYG{n}{pool}\PYG{p}{)}\PYG{p}{,} \PYG{n}{synist}\PYG{p}{(}\PYG{n}{pool}\PYG{p}{)} \PYG{p}{]}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{l+m+mi}{0} \PYG{l+m+mi}{0} \PYG{l+m+mi}{1} \PYG{l+m+mi}{1}
\PYG{p}{[}\PYG{l+m+mi}{0} \PYG{l+m+mi}{0} \PYG{l+m+mi}{1} \PYG{l+m+mi}{1}\PYG{p}{]}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{c+c1}{\PYGZsh{} Define iterator}

\PYG{k}{def} \PYG{n+nf}{iter}\PYG{p}{(}\PYG{n}{initpool}\PYG{p}{,}\PYG{n}{n}\PYG{p}{)}\PYG{p}{:} \PYG{c+c1}{\PYGZsh{} initpool: the inital pool; n: number of iterations}

    \PYG{n}{poolM} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{array}\PYG{p}{(}\PYG{n}{initpool}\PYG{p}{)}\PYG{p}{;}
    \PYG{n}{stateM} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{array}\PYG{p}{(}\PYG{p}{[}\PYG{n}{poolM}\PYG{p}{]}\PYG{p}{)}\PYG{p}{;}


    \PYG{k}{for} \PYG{n}{i} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{n}{n}\PYG{p}{)}\PYG{p}{:}
        \PYG{n}{poolM} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{array}\PYG{p}{(} \PYG{p}{[} \PYG{n}{avgist}\PYG{p}{(}\PYG{n}{poolM}\PYG{p}{)}\PYG{p}{,} \PYG{n}{pesist}\PYG{p}{(}\PYG{n}{poolM}\PYG{p}{)}\PYG{p}{,} \PYG{n}{optist}\PYG{p}{(}\PYG{n}{poolM}\PYG{p}{)}\PYG{p}{,} \PYG{n}{synist}\PYG{p}{(}\PYG{n}{poolM}\PYG{p}{)} \PYG{p}{]} \PYG{p}{)}
        \PYG{n}{stateM} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{append}\PYG{p}{(} \PYG{n}{stateM}\PYG{p}{,} \PYG{n}{np}\PYG{o}{.}\PYG{n}{array}\PYG{p}{(}\PYG{p}{[}\PYG{n}{poolM}\PYG{p}{]}\PYG{p}{)} \PYG{p}{,}\PYG{n}{axis}\PYG{o}{=}\PYG{l+m+mi}{0} \PYG{p}{)}

    \PYG{k}{return} \PYG{n}{stateM}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n+nb}{print}\PYG{p}{(}\PYG{n+nb}{iter}\PYG{p}{(}\PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{,}\PYG{l+m+mi}{1}\PYG{p}{,}\PYG{l+m+mi}{0}\PYG{p}{,}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{,}\PYG{l+m+mi}{10}\PYG{p}{)}\PYG{p}{,} \PYGZbs{}
    \PYG{n+nb}{iter}\PYG{p}{(}\PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{,}\PYG{l+m+mi}{1}\PYG{p}{,}\PYG{l+m+mi}{0}\PYG{p}{,}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{,}\PYG{l+m+mi}{10}\PYG{p}{)}\PYG{p}{,} \PYGZbs{}
      \PYG{n+nb}{iter}\PYG{p}{(}\PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{,}\PYG{l+m+mi}{0}\PYG{p}{,}\PYG{l+m+mi}{1}\PYG{p}{,}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{,}\PYG{l+m+mi}{10}\PYG{p}{)}
     \PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{p}{(}\PYG{n}{array}\PYG{p}{(}\PYG{p}{[}\PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{,} \PYG{l+m+mi}{1}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{,}
       \PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{1}\PYG{p}{,} \PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{,}
       \PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{1}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{]}\PYG{p}{,}
       \PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{]}\PYG{p}{,}
       \PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{]}\PYG{p}{,}
       \PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{]}\PYG{p}{,}
       \PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{]}\PYG{p}{,}
       \PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{]}\PYG{p}{,}
       \PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{]}\PYG{p}{,}
       \PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{]}\PYG{p}{,}
       \PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{]}\PYG{p}{]}\PYG{p}{)}\PYG{p}{,} \PYG{n}{array}\PYG{p}{(}\PYG{p}{[}\PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{,} \PYG{l+m+mi}{1}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{,}
       \PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{1}\PYG{p}{,} \PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{,}
       \PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{1}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{]}\PYG{p}{,}
       \PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{]}\PYG{p}{,}
       \PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{]}\PYG{p}{,}
       \PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{]}\PYG{p}{,}
       \PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{]}\PYG{p}{,}
       \PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{]}\PYG{p}{,}
       \PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{]}\PYG{p}{,}
       \PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{]}\PYG{p}{,}
       \PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{]}\PYG{p}{]}\PYG{p}{)}\PYG{p}{,} \PYG{n}{array}\PYG{p}{(}\PYG{p}{[}\PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{1}\PYG{p}{,} \PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{,}
       \PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{1}\PYG{p}{,} \PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{,}
       \PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{1}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{]}\PYG{p}{,}
       \PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{]}\PYG{p}{,}
       \PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{]}\PYG{p}{,}
       \PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{]}\PYG{p}{,}
       \PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{]}\PYG{p}{,}
       \PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{]}\PYG{p}{,}
       \PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{]}\PYG{p}{,}
       \PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{]}\PYG{p}{,}
       \PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{]}\PYG{p}{]}\PYG{p}{)}\PYG{p}{)}
\end{sphinxVerbatim}


\section{Factor Graph}
\label{\detokenize{topics/factor-graph::doc}}\label{\detokenize{topics/factor-graph:factor-graph}}
In the propagation of information, we are usually dealing with such relations that some system (factor) \(f\) is related to some inputs (variable) \(x\).

Factor graph builds such networks of relations. In such a graph, we use boxes to represent factors, and circles to represent variables.

\begin{figure}[htbp]
\centering
\capstart

\noindent\sphinxincludegraphics{{factor-graph-wikipedia}.jpg}
\caption{From \sphinxhref{https://en.wikipedia.org/wiki/Factor\_graph\#/media/File:Factorgraph.jpg}{Factor Graph @ Wikipedia}.}\label{\detokenize{topics/factor-graph:id2}}\end{figure}

Suppose we need to find out the probability of factor \(f_4\), we have to take in the two variables \(X_2\) and \(X_3\).

One important question is that we can actually calculate the probability of a specific configuration of the variables, which is
\begin{equation*}
\begin{split}P \propto f_1(X_1) + f_2(X_1,X_2) + f_3(X_1,X_2) + f_4(X_2,X_3).\end{split}
\end{equation*}
In general, we can assign weights \(w_i\) to each factors \(f_i\), so that the probability of each set of variables is
\begin{equation*}
\begin{split}P(X_1,X_2,X_3,X_4) \propto w_1 f_1(X_1) + w_2 f_2(X_1,X_2) + w_3 f_3(X_1,X_2) + w_4 f_4(X_2,X_3).\end{split}
\end{equation*}
The normalization is the partition function, i.e., the summation of all the possible combinations of variables,
\begin{equation*}
\begin{split}Z = \sum_{\{X_1,X_2,X_3,X_4\}} P(X_1,X_2,X_3,X_4).\end{split}
\end{equation*}
As an example, we consider \(X_i\) is 0 or 1. The combinations are (partially) listed below.


\begin{savenotes}\sphinxattablestart
\centering
\sphinxcapstartof{table}
\sphinxcaption{List of Possible Configurations of Variables}\label{\detokenize{topics/factor-graph:id3}}
\sphinxaftercaption
\begin{tabulary}{\linewidth}[t]{|T|T|T|T|T|T|}
\hline

\(X_1\)
&
0
&
1
&
1
&
1
&
…
\\
\hline
\(X_2\)
&
0
&
0
&
1
&
1
&
…
\\
\hline
\(X_3\)
&
0
&
0
&
0
&
1
&
…
\\
\hline
\(X_4\)
&
0
&
0
&
0
&
0
&
…
\\
\hline
\end{tabulary}
\par
\sphinxattableend\end{savenotes}


\subsection{References and Notes}
\label{\detokenize{topics/factor-graph:references-and-notes}}\begin{enumerate}
\item {} 
\sphinxhref{https://en.wikipedia.org/wiki/Factor\_graph}{Factor Graph @ Wikipedia}.

\item {} 
\sphinxhref{http://deepdive.stanford.edu/assets/factor\_graph.pdf}{DeepDive Docs}.

\end{enumerate}


\chapter{Some Random Thoughts}
\label{\detokenize{random-thoughts::doc}}\label{\detokenize{random-thoughts:some-random-thoughts}}

\section{Pan-Intelligence}
\label{\detokenize{random-thoughts:pan-intelligence}}
Biological neural networks give us human intelligence. Are biological neural networks special out of the numerous different kinds of network systems? I prefer to believe they are not special.

So what is a list of all systems that could function as a neural network?
\begin{enumerate}
\item {} 
Social networks. We could model the society as an information processing network. It is extremely complicated, at least more complicated than the models in physics.

\item {} 
The whole ecological system on the Earth is like a web of lives.

\item {} 
Networks that connects time instead of space is also an interesting idea. If we have nodes \(N_i\) located at different time \(t_i\) and the nodes are connected. Now we have a network which can be analysis. However, for casuality reasons we should limit the network to be directed.

\end{enumerate}


\section{Why Can We Understand the Universe?}
\label{\detokenize{random-thoughts:why-can-we-understand-the-universe}}
Our brain contains significantly less degrees of freedom than the whole universe. Yet we are developing very good understanding of whole universe.

One of the interesting points is that the universe doesn’t go through all the possible states of its configuration. There are many constraints on the universe that forbids it from developing into many other states. Thus we could understand the restrictions thus grasp the key of the state of the universe.

In some sense, intelligence should have this ability to find the restrictions of the environment and develop understanding of how the enviroment changes. Without the understanding of the restrictions of the enormously complex enviroment, it won’t be possible to predict.


\chapter{References}
\label{\detokenize{ref:references}}\label{\detokenize{ref::doc}}

\chapter{Acknowledgement}
\label{\detokenize{acknowledgement::doc}}\label{\detokenize{acknowledgement:acknowledgement}}
I am grateful to the authors of Sphinx.

My wife Han Lu helps me a lot in this set of learning notes. She provided topics, ideas, and organized a reading club on the topic of neuroscience, all of which set my path to learn more about intelligence.

\begin{sphinxthebibliography}{BrainConnectivity}
\bibitem[Tetzlaff2012]{\detokenize{Tetzlaff2012}}{\phantomsection\label{\detokenize{neuroscience/memory/time-scale:tetzlaff2012}} 
Tetzlaff, C., Kolodziejski, C., Markelic, I., \& Wörgötter, F. (2012). Time scales of memory, learning, and plasticity. Biological Cybernetics, 106(11\textendash{}12), 715\textendash{}726. \sphinxurl{https://doi.org/10.1007/s00422-012-0529-z}
}
\bibitem[Tetzlaff2013]{\detokenize{Tetzlaff2013}}{\phantomsection\label{\detokenize{neuroscience/memory/time-scale:tetzlaff2013}} 
Tetzlaff, C., Kolodziejski, C., Timme, M., Tsodyks, M., \& Wörgötter, F. (2013). Synaptic Scaling Enables Dynamically Distinct Short- and Long-Term Memory Formation. PLoS Computational Biology, 9(10), e1003307. \sphinxurl{https://doi.org/10.1371/journal.pcbi.1003307}
}
\bibitem[LaFerla2008]{\detokenize{LaFerla2008}}{\phantomsection\label{\detokenize{neuroscience/alzheimers-disease:laferla2008}} 
\sphinxhref{http://www.nature.com/nrn/journal/v8/n7/full/nrn2168.html}{Intracellular amyloid-\(\beta\) in Alzheimer’s disease} (Figure 3 is a nice illustration of the process), \sphinxhref{http://www.nature.com/nrn/posters/ad/index.html}{Poster: Amyloid-\(\beta\) and tau in Alzheimer’s disease}
}
\bibitem[Jaunmuktane2015]{\detokenize{Jaunmuktane2015}}{\phantomsection\label{\detokenize{neuroscience/alzheimers-disease:jaunmuktane2015}} 
\sphinxhref{https://www.nature.com/nature/journal/v525/n7568/full/nature15369.html}{Evidence for human transmission of amyloid-\(\beta\) pathology and cerebral amyloid angiopathy}
}
\bibitem[Palop2016]{\detokenize{Palop2016}}{\phantomsection\label{\detokenize{neuroscience/alzheimers-disease:palop2016}} 
\sphinxhref{http://www.nature.com/nrn/journal/v17/n12/full/nrn.2016.141.html}{Network abnormalities and interneuron dysfunction in Alzheimer disease}
}
\bibitem[Newman2016]{\detokenize{Newman2016}}{\phantomsection\label{\detokenize{neuroscience/functional-connectivity:newman2016}} 
Newman, M. E. J. (2016). \sphinxhref{http://arxiv.org/abs/1606.02319}{Community detection in networks: Modularity optimization and maximum likelihood are equivalent}. ArXiv 1606.02319, 1\textendash{}8.
}
\bibitem[Zhang2014]{\detokenize{Zhang2014}}{\phantomsection\label{\detokenize{neuroscience/functional-connectivity:zhang2014}} 
Zhang, P., \& Moore, C. (2014). \sphinxhref{http://doi.org/10.1073/pnas.1409770111}{Scalable detection of statistically significant communities and hierarchies using message passing for modularity}, 111(51), 18144\textendash{}18149.
}
\bibitem[Boguna2010]{\detokenize{Boguna2010}}{\phantomsection\label{\detokenize{neuroscience/functional-connectivity:boguna2010}} 
Boguna, M., Papadopoulos, F., \& Krioukov, D. (2010). \sphinxhref{http://doi.org/10.1038/ncomms1063}{Sustaining the Internet with Hyperbolic Mapping. Nature Communications}, 1(6), 1\textendash{}8.
}
\bibitem[BrainConnectivity]{\detokenize{BrainConnectivity}}{\phantomsection\label{\detokenize{neuroscience/functional-connectivity:brainconnectivity}} 
\sphinxhref{http://www.scholarpedia.org/article/Brain\_connectivity}{Brain connectivity}
}
\bibitem[Sivakumar2016]{\detokenize{Sivakumar2016}}{\phantomsection\label{\detokenize{neuroscience/signal-decomposition:sivakumar2016}} 
Sivakumar SS, Namath AG and Galán RF (2016) \sphinxhref{http://journal.frontiersin.org/article/10.3389/fncom.2016.00059/full}{Spherical Harmonics Reveal Standing EEG Waves and Long-Range Neural Synchronization during Non-REM Sleep} . Front. Comput. Neurosci. 10:59. doi: 10.3389/fncom.2016.00059
}
\bibitem[GErmentrout]{\detokenize{GErmentrout}}{\phantomsection\label{\detokenize{neuroscience/dispersion-relation:germentrout}} 
\sphinxstyleemphasis{Mathematical Foundations of Neuroscience} Chapter 6.
}
\bibitem[Lagzi2015]{\detokenize{Lagzi2015}}{\phantomsection\label{\detokenize{neuroscience/misc:lagzi2015}} 
\sphinxhref{https://www.bcf.uni-freiburg.de/people/papers-rotter/lagzi-2015-plos-one.pdf}{Dynamics of competition between subnetworks of spiking neuronal networks in the balanced state}
}
\bibitem[Deco2010]{\detokenize{Deco2010}}{\phantomsection\label{\detokenize{neuroscience/misc:deco2010}} 
\sphinxhref{http://journals.sagepub.com/doi/10.1177/1073858409354384}{The Dynamical Balance of the Brain at Rest}
}
\bibitem[Deco2014]{\detokenize{Deco2014}}{\phantomsection\label{\detokenize{neuroscience/misc:deco2014}} 
\sphinxhref{http://www.cell.com/neuron/fulltext/S0896-6273(14)00735-1}{Great Expectations: Using Whole-Brain Computational Connectomics for Understanding Neuropsychiatric Disorders}
}
\bibitem[Gerstner2002]{\detokenize{Gerstner2002}}{\phantomsection\label{\detokenize{equation-solving-in-neuroscience/green-function:gerstner2002}} 
Wulfram Gerstner and Werner M. Kistler, Spiking Neuron Models, (2002).
}
\bibitem[Deco2008]{\detokenize{Deco2008}}{\phantomsection\label{\detokenize{topics/mean-field:deco2008}} 
Deco, G., Jirsa, V. K., Robinson, P. A., Breakspear, M., \& Friston, K. (2008). \sphinxhref{https://doi.org/10.1371/journal.pcbi.1000092}{The Dynamic Brain: From Spiking Neurons to Neural Masses and Cortical Fields}. PLoS Computational Biology, 4(8), e1000092.
}
\end{sphinxthebibliography}



\renewcommand{\indexname}{Index}
\printindex
\end{document}